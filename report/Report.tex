\documentclass[11pt, a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage[parfill]{parskip} % Empty line instead of indentation
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage{listings}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{eurosym} %Euro symbol
%\usepackage[ngerman]{babel}
\usepackage{cancel} % Cancel fractions
\usepackage{units}
\usepackage{xcolor}
\usepackage{setspace}
\usepackage{subcaption}
\usepackage{algpseudocode} % Pseudocode
\usepackage{algorithm} % Float for algorithms
\usepackage{multirow}

% TODO replace some of these \newcommands with \DeclareMathOperators
\newcommand\braces[1]{\left(#1\right)}
\newcommand\brackets[1]{\left[#1\right]}
\renewcommand{\vec}[1]{\underline{#1}}
\newcommand{\mat}[1]{\underline{\underline{#1}}}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand\tr[1]{\mathrm{tr}\br{#1}}
\newcommand\average[1]{\left\langle#1\right\rangle}
\newcommand{\acos}[1]{\mathrm{acos}\braces{#1}}
\newcommand{\asin}[1]{\mathrm{asin}\braces{#1}}
\newcommand{\intend}[1][]{\ \mathrm{d}#1}
\newcommand{\derivative}[2][]{\ \frac{\mathrm{d}#1}{\mathrm{d}#2}} %\derivative[a]{b}
\newcommand\expectedValue[1]{\mathbb{E}\braces{#1}}
\newcommand\variance[1]{\mathbb{V}\braces{#1}}
\newcommand\setequal{\overset{!}{=}}
\newcommand{\gerquote}[1]{\glqq#1\grqq}
\DeclareMathOperator{\sign}{sign}
\definecolor{AI-BLUE}{rgb}{0,0.57,0.87}

\onehalfspacing
\setlength\parindent{0pt}
\allowdisplaybreaks

\title{TITLE}
\author{AUTHOR}
\date{\today}

\makeatletter
    \setlength\@fptop{0\p@}
\makeatother

\usepackage{amsmath}
\begin{document}
\thispagestyle{empty}

\begin{titlepage}
    \begin{center}
    \vphantom{1cm}
    \vspace{1cm}
    \huge \textbf{Exploring ways to harden deep convolutional neural networks against constructed adversarial examples} \\
    \vspace{3cm}
    %\LARGE \textbf{Report}\\
    \normalsize
    Written Master Thesis Report \\
    for the Masters Program of \textcolor{AI-BLUE}{[Applied Computer Science]}\\
    at the Ruhr-University Bochum\\
    in the Summer Term 2016\\
    \vspace{3cm}
    %\normalsize
    \textbf{Submitted by}\\
    B. Sc. Christian Andreas Mielers (108 011 204 956)\\
    \vspace{1cm}
    December 5, 2016 \\
    \vspace{3cm}
    \textbf{Supervisors} \\
    PD Dr. Rolf P. WÃ¼rtz \\
    M. Sc. Andreas Nilkens
    \end{center}
\end{titlepage}

\newpage
\pagenumbering{arabic}
\setcounter{page}{2}

\tableofcontents

\newpage
\section{Introduction}
\label{sec:introduction}

As the problems people try to solve with the aid of computers become more and more complex, their ability to write software that can keep up with those aspirations does not follow suit. Solving more complex problems with software does not only incur the cost of finding viable solutions and formalizing them in computer code, but also the cost of integrating all the necessary components, organizing their interactions, ensuring they use common resources in a well-defined manner, and testing all of the above. These tasks grow essentially super-linearly in difficulty, and can thus quickly become harder than the problem that the system was initially meant to solve.

One way out of this dilemma that people turn to increasingly often is the usage of machine learning techniques. The goal of machine learning is to free people from the burden of finding and programming a solution. In order to do so, a \emph{model} is defined which embodies the structure of a possible solution on a highly abstract level. However, such a model as potentially a myriad of \emph{parameters}, and no knowledge of reasonable values for those parameters can be assumed. What gives machine learning its name and makes it feasible despite the many unknown parameters is the learning method. This is a method to find a working configuration of parameters based on \emph{training data}. In \emph{supervised learning}, the algorithm that performs the learning is provided numerous pairs of an exemplary input and the corresponding desired output that the model should deliver. From these pairs, it extracts, or \emph{learns}, the relationship between input and output data, and codifies it in the model's parameters. In this manner, the need for a handcrafted solution is eliminated. The downside to this approach is that learning algorithms usually require a copious amount of training data to find good parameters. However, this drawback is increasingly often offset by the reduced software complexity, since  there is ready-made machine learning software available and data is becoming easier to access.

% TODO substantiate claim that ANNs are among the top machine learning models
Among some of the most momentous machine learning models are \emph{artificial neural networks (ANNs)}. They are inspired by discoveries from the field of neural biology and incorporate them into models that, on a very abstract level, resemble brain structures on the detail level of individual neurons. The workings of these  neurons are greatly simplified, but in this way they can be easily connected in large bulks, empowering the resulting network to solve difficult tasks with impressive precision. ANNs are mainly used for two types of tasks: \emph{classification} and \emph{regression}. While the latter is concerned with learning a function $f: X \rightarrow Y$ that maps input data to the desired output values, it is the former that will be of interest within the scope of this thesis. Classification instead aims to assign each input to one category out of a predefined set of categories. The data used to train the network is structured accordingly: it consists of the input data and a \emph{label} that denotes its class.
% TODO note that learner for neural networks is "gradient descent"?

% TODO illustrate image classification
One area where neural networks are particularly successful is \emph{image classification}, the task of assigning labels to images based on the objects contained in it. A big part of the success here can be attributed to a special variant of ANNs, called \emph{Convolutional Neural Networks (CNNs)}. These networks focus on local features in the input by applying \emph{filters} in a convolution operation, thereby curtailing redundancies in the free parameters. Combined with the focus on local features, this efficient use of free parameters allows for much larger and thus more powerful networks.

% TODO illustrate training and test steps with an image
When training a neural network for an image classification task, one desires a CNN to not only classify the images used for training correctly, but to infer and learn the fundamental characteristics of each class. Only then will the network be able to \emph{generalize} its classification capabilities to the previously unseen images it will encounter in productive use. To gauge a network's ability to generalize, another data set, called the \emph{test set} is usually withheld while training the model. The experimenter can then evaluate the performance on this set outside of training, thus obtaining an estimator for field performance of the model.

In the past, ANNs in general and CNNs in particular have shown promising results not only on training data but also test data. However, recent discoveries have indicated that the generalization capability of most neural networks is not as strong as one might guess from test set performance. While networks certainly can classify unseen input with high reliability, it is possible to find minute transformations of correctly classified test data, on which the network will predict another class with high confidence. The changes that need to be applied for this to work are miniscule and undetectable to the human eye. The resulting images are called \emph{adversarial examples (AEs)}.

In the initial publication a rather complex method for creating those images was presented. A major impediment of this method was the excessive computational power and time required to produce AEs. Fortunately, a follow-up paper greatly improved the feasibility of this process by introducing a gradient-based procedure for generating AEs. In that manner a considerable amount of AEs can be generated in a reasonable time frame, enough to allow for reasonable quantitative analysis of the AE's properties. Therefore, this thesis will look at various statistics of larger groups of AEs, including their frequency domain spectra and the ways these spectra differ from those of unadulterated images. Regarding the spectra, we will also look at the distribution of values within the spectra with regards to the frequency and orientation.

Furthermore, the process of generating adversarial examples can be inspected on a considerably larger scale. Plots for the network confidence on images in the process of iteratively being turned into AEs will provide insights on the process with regards to interation count, and serve as the foundation for attempts to predict whether an image can be successfully made adversarial. Data on the success rate of converting images to adversarial examples will be presented, both per original and adversarial class as well as overall figures.

The aforementioned results will be obtained with multiple data sets and various network layouts and parameter configurations. The latter will also allow for inspection of the impact of parameter choice on various statistics.

% TODO
% History
% Industry applications
% Future prospects










\newpage
\section{Artificial Neural Networks}
\label{sec:artificial-neural-networks}
In the area of machine learning, artificial neural networks are some of the most widely studied models. They stem from insights into the workings of real, biological brains in the field of neural biology. Here, the individual neuron is used as the fundamental unit of computation. A schematic version of the biological variant is shown in figure \ref{fig:biological-neuron-schematic}.

\begin{figure}[h!tb]
	\centering
	\includegraphics[width=0.5\textwidth]{images/biological_neuron.png}
	\caption[Biological neuron]{Schematic of a biological neuron, taken from \cite{biological-neuron-schematic}}
	\label{fig:biological-neuron-schematic}
\end{figure}

A neuron receives incoming electrical pulses, called \emph{action potentials}, from the axons of other neurons via its dendrites. A connection between an axon and a dendrite is called a \emph{synapse}. These synapses can vary in strength, so the intensity of the received pulse is modulated by the synaptic connectivity. Multiple action potentials may arrive in the soma, the cell body of a neuron, in quick succession, causing the membrane potential of the soma to increase. If enough action potentials arrive within a short time via strong synapses, this may cause the cell in question to initiate an action potential which traverses along its axon. This can be considered to be a decision of the cell to pass the message on to other cells.

An artificial neuron as used in an ANN mimics this process, but in a greatly simplified form. All action potential rates are represented by real numbers\footnote{Floating point numbers in a computer system}. A visual representation is provided in figure \ref{fig:artificial-neuron-schematic}.


\begin{figure}[h!tb]
	\centering
	\includegraphics[width=0.45\textwidth]{images/artificial_neuron.png}
	\caption[Artificial neuron]{Schematic of an artificial neuron}
	\label{fig:artificial-neuron-schematic}
\end{figure}

The first thing to note is that the input to the neuron is now formalized as a vector $\vec{i}$ with components $i_1 \dots i_n$, where $n$ represents the number of inputs that influence this neuron. Each input value $i_j$ is multiplied with a \emph{weight}, denoted by $w_j$. This weight can be thought of as an abstraction of the synaptic strength between two neurons. Then, all of these input-weight pairs are summed up to realize the counterpart to the membrane potential, labeled $m$. To form the neuron's output $o$, one last step is neccessary. In biological neurons, the rate of outgoing spikes of a neuron is not linear in the incomming spikes. Thus, a nonlinear \emph{activation function} is applied to the membrane potential $m$, symbolized by the sigmoidally shaped line in the rectangular box in figure \ref{fig:artificial-neuron-schematic}. If that activation function is called $\sigma$, the mathematical operation performed by a single neuron can be summarized according to equations \eqref{eq:neuron-math-membrane-potential} and \eqref{eq:neuron-math-activation-function}.

\begin{align}
	m &= \sum_{i=1}^n w_n \cdot i_n = \vec{w}^T \vec{i} \label{eq:neuron-math-membrane-potential} \\
	o &= \sigma \braces{m} \label{eq:neuron-math-activation-function}
\end{align}

Take note of the fact that the very first step performed on the input is essentially an inner product. This will become important later on, especially in chapter \ref{sec:adversarial-examples}. Several activation functions are possible. A plot of 3 common choices is given in figure \ref{fig:activation-functions}.

\begin{figure}[h!tb]
	\centering
	\includegraphics[width=0.6\textwidth]{images/activation_functions.png}
	\caption[Activation functions]{Logistic sigmoidal, hyperbolic tangent and ReLU activation functions}
	\label{fig:activation-functions}
\end{figure}

Out of these, the easiest to justify is the logistic sigmoid. If the output is to represent the neuron's firing rate, it must not become negative, and it must saturate because of the refractory period required by biological cells. Under these conditions, the logistic function is a simple continuous choice. The hyperbolic tangent is a linearly transformed version of this. While its negative range may be harder to interpret biologically, the option to output negative values may relieve the trainable parameters of a network of some demand for computational power, thus facilitating training. Despite being non-continuous and having no upper bound, the ReLU is the most popular activation function according to \cite{deep-learning}. The networks utilized in this thesis mostly make use of the $\tanh$ and ReLU activation functions.

Since a single neuron as described above consists only of an inner product and a one-dimensional non-linearity, its computational power is limited. Artificial neural networks rely on thousands of neurons to perform their tasks. This gives rise to the question of how to connect all those neurons. Conceptually, one could link them up in such a way that a neuron receives input from another neuron while at the same time delivering input to that very neuron (either directly or via proxy neurons), forming a cycle. This is sometimes done and is called a \emph{Recurrent Neural Network (RNN)}, but is out of scope for this thesis. Instead, neurons will be organized in \emph{layers}, as shown in figure \ref{fig:neural-nework-structure}.

% TODO fix indices
\begin{figure}[h!tb]
\centering
\includegraphics[width=\textwidth]{images/network_layout.png}
\caption{Structure of neuron arrangement in an ANN}
\label{fig:neural-nework-structure}
\end{figure}

% TODO use top-down image instead of left-right. Requires to reformulate the text a bit.
Here, each layer contains numerous neurons, none of which share a connection. This means that there is no interaction within a layer and thus it seems reasonable to inspect the actions of a network in a layer-by-layer fashion. The layer on the left is often called the "first" or "lowest" layer, and it is the location where the input data is fed into the network. Therefore, its neurons deviate from the description given in figure \ref{fig:artificial-neuron-schematic} and equations \eqref{eq:neuron-math-membrane-potential} and \eqref{eq:neuron-math-activation-function}. No computation takes place here, the data is taken as-is and used as input for the layer above.

% TODO describe layers as homeomorphisms according to http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/
In the figure, the layer above the input contains 3 neurons and is named "Hidden layer 1". Hidden layers are those that are neither input nor output and thus do not communicate with any system outside the network. Note that each neuron here has two indices: The first one designates the layer affiliation and the second one identifies the individual neuron. The first hidden layer is followed by a second one. A neural network can conceptually have an unlimited amount of hidden layers, but at least one is necessary to satisfy the \emph{universal approximation theorem} \cite{universal-approximator-theorem} which attests that for every function, there exists a neural network that can approximate said function to an arbitrary but fixed accuracy\footnote{However, this is of limited practical use, since the theorem makes no statement about the required number of neurons, or how to set the weights.}.

A further notational simplification can be made at this point. Equation \eqref{eq:neuron-math-membrane-potential} described the operation of a single neuron as an inner product. If we treat the output of \emph{Hidden layer 1} as a vector $\vec{x_0}$ and the output of \emph{Hidden layer 2} as another vector $\vec{x_1}$, the entire operation of the second hidden layer can be described by a matrix-vector product according to equations \eqref{eq:layer-math-membrane-potential} and \eqref{eq:layer-math-activation-function}.

\begin{align}
	\vec{m_1} &= \mat{W_{1}} \, \vec{x_0} \label{eq:layer-math-membrane-potential} \\
	\vec{o_1} &= \sigma \braces{\vec{m_1}} \label{eq:layer-math-activation-function}
\end{align}

where $\sigma \braces{\vec{m_1}}$ indicates the element-wise application of the activation function to the membrane potentials. If the weight vectors used in equation \eqref{eq:neuron-math-membrane-potential} are used as row vectors of $\mat{W_1}$, then this is an equivalent description on a higher level of abstraction. This can of course be done analogously for the other layers of the network.

Beyond biological plausibility, this formalism provides another justification for using activation functions at all. If they were not part of a neural network model, each layer would solely perform a matrix-vector product, which, due to their linear properties, could be collapsed into a single matrix-vector product, greatly reducing the model's capacity to describe complex relations between input data and output data.

The last layer generates the output of the network. It too can in part be described as a matrix-vector product. It may also share the same activation function as other layers, in which case its functionality is mostly identical with hidden layers. However, this is sometimes inappropriate. In regression, for instance, the range of the function to be learned may go far into the negative range, well below $-1$. None of the activation functions shown in figure \ref{fig:activation-functions} are capable of modelling this. Similarly, in classification, it might be desirable for the output to represent the probabilities of an input belonging to each of the classes. While all of the discussed activation functions include the range $[0-1]$, thereby making such a representation possible, other activation functions specifically suited for output layers, like the \emph{softmax function}, can force those properties.

% TODO Deep dream
% TODO Akin to layered processing in vision
The relatively recent machine learning trend going by the name of \emph{deep learning} builds extensively on this layered architecture of neural networks. Specifically, \emph{deep} neural networks use many more hidden layers than just 1, in contrast to the preceding networks, which were ex post named \emph{shallow} networks. The central hypothesis of this approach is that the various layers of a network will learn to perform distinct, hierarchically structured functions. Typically, the first layer (or first few layers) are expected to become low-level feature extractors, providing basic information for the higher layers. In vision tasks, for example, one can often observe that the lowest layer works as a corner detector, because the geometry within an image often contains the most important information. The following layers then hopefully extract features with an increasing level of abstraction, e.g. combining corners to edges and more complex shapes and contours. The last layer is tasked with making the final decision regarding the input object's class, using the presence of those high-level features as the basis for its choice.

% TODO some networks make use of this? Sure its not all of them?
% TODO glorot formula
% TODO caffe does glorot initialization slightly differently
One question that has not been addressed yet is how to actually set the weights. Under normal circumstances there is no closed form solution to compute the optimal parameters for a model. Thus, a training process that starts with a random set of weights and then iteratively improves on them is used. To accomplish random initialization, values for all weights are drawn from one of various distributions. Common choices are uniform and normal distributions. In 2010, a more sophisticated method has been proposed in \cite{glorot-understanding-deep-nn-training}. It still relies on a uniform initialization, but the limits are set for each layer individually, so as to retain the variance of the data as it flows through the layers of the network. Some networks in this thesis make use of this advanced initialization\footnote{The \emph{caffe} software used actually slightly deviates from the proposed process, see \cite{caffe-xavier}}.

In order to improve the thusly initialized weights, it is first necessary to define a way to quantify how well (or poorly) a given set of parameters performs. To this end, a \emph{loss function} (also called \emph{objective function} or \emph{error function}) is used, which combines the predictions of a network with the training labels to provide an estimate of its performance. In other words, it is a function that maps labeled input data and the parameters to a scalar value. A common loss function is the \emph{Mean Squared Error (MSE)} given in equation \eqref{eq:mse}, where $N$ is the number of data points and $\vec{\hat Y}$ and $\vec{Y}$ are vectors containing the predictions and true labels, respectively. In multinomial classification tasks, the \emph{cross entropy} loss given in equation \eqref{eq:cross-entropy-loss} \cite{caffe-cross-entropy} is a good choice if the predictions for all classes are to be interpreted as probabilities (meaning they are required to add up to 1). Here, $\vec{\hat Y}$ is a matrix containing all predicted class probabilities for all $N$ data points and $\vec{Y}$ is a vector indicating the true class.

% TODO make this more readable
\begin{align}
	\text{MSE} &= \frac{1}{N} \sum_{i=1}^N \braces{\vec{\hat Y}_i - \vec{Y}_i}^2 \label{eq:mse} \\
	\text{Cross entropy} &= \frac{-1}{N} \sum_{i=1}^N \log{\braces{\mat{\hat Y}_{i, \vec{Y}_i}}} \label{eq:cross-entropy-loss}
\end{align}

The goal of neural network training is usually to minimize the loss function (although it could be maximization depending on the formulation of the loss function). This is most often accomplished with a \emph{stochastic gradient descent} \cite{stochastic-gradient-descent-learning-neural-networks} on the loss function mentioned above w.r.t the weights of the network\footnote{In practice, the gradient w.r.t a lot of weights can be computed very efficiently by exploiting properties of the chain rule, a method known as \emph{Backpropgation}, or Backprop for short.}. A simple, one-dimensional case is given in figure \ref{fig:gradient-descent}.

\begin{figure}[h!tb]
	\centering
	\includegraphics[width=0.7\textwidth]{images/gradient_descent.png}
	\caption[Gradient descent method]{Illustration of the gradient descent method}
	\label{fig:gradient-descent}
\end{figure}

Starting at an arbitrary point, the gradient always points in the direction of steepest ascent. The idea of a gradient descent is to move the parameters (in this case the weights) into the opposite direction in order to descend the slope of the loss function. Following the left-oriented green arrow, the network with parameters corresponding to the blue dot would adopt the parameters corresponding to the transparent variant slightly to the left, resulting in a reduced loss. This process is iterated many times in order to hopefully reach a minimum that represents a good solution to the training process.

The magnitude of the gradient is usually used as an indication of how much the weights should be changed. Note that there can be many complications, for instance, if the blue dot had started to the left of the local maximum near the left side, it might have found a smaller local minimum than the one marked in figure \ref{fig:gradient-descent}.

% TODO
% Neural networks discard time









\subsection{Convolutional Neural Networks}
A special kind of ANN that has proven to be particularly effective on image data is the \emph{Convolutional Neural Network (CNN)}. All networks used in this thesis are CNNs. They differ from conventional networks in the way some of the layers function, specifically, they contain \emph{convolutional layers}, which, as the name implies, perform convolutions on the incoming data. In a way, the weight matrix that connects the layer's neurons with the neurons of the previous layer is replaced with \emph{kernels} or \emph{filters}, with which the incoming data is convolved. A schematic representation of this procedure is given in figure \ref{fig:convolutional-layer}.

\begin{figure}[h!tb]
	\centering
	\includegraphics[width=0.8\textwidth]{images/convolution_layer.png}
	\caption[Functionality of convolutional layers]{Schematic of a convolutional layer's functionality}
	\label{fig:convolutional-layer}
\end{figure}

The easiest way to understand this is to imagine the left part as an input image of size $4 \times 8$, with an immediately following convolutional layer. In this example, the convolutional layer has one filter of size $3 \times 3$, though other sizes are possible. Since these are 2 arrays of two-dimensional shape, the computation of the convolutional layer's output can be computed by a two-dimensional convolution, save for possible activation functions. As can be seen in figure \ref{fig:convolutional-layer}, this produces an output of size $2 \times 6$. The size reduction is a consequence of neglect to deal with the edges of the input image; the filter is only centered on those areas where the input is completely defined\footnote{While the reduction from $4 \times 8$ to $2 \times 6$ may seen extreme, it is worth mentioning that the number of pixels "lost" only grows with the \emph{circumference} of the input data, so for a more reasonably sized image, the output map will only be slightly smaller.}.

However, the figure also indicates that there is a different, connectionist perspective on this process. In this view, the neurons on the right still connect to the preceding neurons with weights and compute an inner product. There are 2 major differences to a conventional layer. First, each neuron is only connected to 9 input values, instead of all of them, leading to the first considerable reduction in the number of weights that need to be trained. Second, all neurons use \emph{the same weights} to connect to their (distinct) 9 inputs, giving rise to the term \emph{weight sharing}. This again greatly reduces the number of weights, to a constant count of 9, regardless of the size of the previous layer. For a quick comparison, a conventional layer with 12 neurons would lead to $12 \cdot 32 = 384$ parameters. Additionally, in contrast to the constancy of the 9 weights in a convolutional layer, this figure would grow with the product of both layer sizes. For image sizes typically found in practical applications, this rapidly becomes impractical.

Using a convolutional layer does not just reduce the number of parameters that require training, but also tailors the network's structure to the specific properties of image related tasks. Since each neuron in a convolutional layer is connected only to a small, contiguous region of the input image, the learning algorithm is forced to train the kernel weights in such a way that they focus on local features. Since a lot of distinctive information in images such as edges, corners and textures emerges from local features, this mechanism confers the benefit that data not relevant to those features is implicitly ignored. This shares some similarities with the concept of receptive fields in human vision, which are also local.

The convolutional layers of the CNNs used in this thesis and in practice are slightly more complicated than the above explanation suggests. If a convolutional layer would only use one kernel, it would only be able to extract a single feature from the input data. Because this is insufficient, convolutional layers generally have numerous kernels, each of which gets convolved with the input independently, producing multiple outputs called \emph{maps}. Concordantly, a convolutional layer is able to receive multiple maps as input. The scenario with multiple input and output maps is depicted in figure \ref{fig:convolutional-layer-maps}.

\begin{figure}[h!tb]
	\centering
	\includegraphics[scale=1.00]{images/convolution_layer_maps.png}
	\caption[Input and output maps in convolutional layers]{Schematic of a convolutional layer with multiple input and output maps}
	\label{fig:convolutional-layer-maps}
\end{figure}

On the left side we see 3 input maps. On the lowest layer, the input maps correspond to the 3 color channels red, green and blue\footnote{When using a grayvalue image, there is only 1 channel and therefore only 1 map}. On the right side the convolutional layer produces 2 output maps. The process for computing one such output map is as follows: First of all, each input map gets convolved with a filter that is unique to that map (and to the output map). The resulting arrays are summed up to form one array of the correct shape. Depending on the network layout, processing the array element-wise with an activation function may be a part of this step. This process is performed multiple times with distinct filters, depending on how many output maps are desired. Since for every pair of an input map and an output map there is exactly one filter, the total number of filters in a convolutional layer is $M \cdot N$, where $M$ and $N$ are the counts of input and output maps, respectively.

% TODO Source for Max-Pooling
The parsimony with free parameters required by a convolutional layer to produce an output map makes it feasible to use many filters and consequently produce a great deal of output maps. This empowers the CNN with considerable modeling capacity, but also poses the challenge of dealing with all that generated output. CNNs have one more mechanism at their disposal to solve this specific problem. To reduce the dimensionality of the feature maps, they use \emph{pooling} layers, which execute subsampling on the maps as indicated by figure \ref{fig:pooling-layer}.

\begin{figure}[h!tb]
	\centering
	\includegraphics[scale=1.00]{images/pooling_layer.png}
	\caption[Pooling layer functionality]{Schematic of a pooling layer downsizing an input map}
	\label{fig:pooling-layer}
\end{figure}

The input is located to the left and comprises 32 neurons arranged in a two-dimensional $4 \times 8$ array. The pooling layer condenses the information to 8 neurons in a $2 \times 4$ array. This is done by segmenting the input into regions that do not necessarily need to be disjoint or cover the entire input. Each of these segments covers a small, usually rectangular region of the input. An operation that reduces all values in a segment to a single value is then applied. All scalars from each segment together constitute the output of the pooling layer. Common reduction operations include averaging and taking the maximum. The networks in this thesis exclusively use the latter.

As one can see, the amount of data has been greatly reduced. However, if pooling is applied to the output maps of a convolutional layer, it is reasonable to assume that critical information is retained. Unless the input image contains very high frequency information apart from noise, the convolutions should yield similar values at adjacent points. A max pooling operation then would simply select the best match for a given feature. Besides greatly reducing the amount of data to be handled, this method also confers the benefit that translational invariance is increased since the exact location of a specific feature is discarded by the pooling.

% TODO
% Max-Pooling provides translation invariance










\section{Adversarial Examples}
\label{sec:adversarial-examples}

% TODO might Neural Representation and Neural Computation be a good cite for this?
% TODO is dynamic-field-theory-movement-preparation a good cite?
% TODO cite more things, especially for the molecular modeling
In trying to recreate the learning power of the human brain on a computer, one can focus on various levels of abstraction. A system could potentially model all the chemical processes that happen within and around cells and their synapses on a molecular level. Less drastically, one could work with aggregates and averages of these values in order to retain most of the biological components and create a time-dependent system this way. Alternatively, larger groups of neurons could be used as the basic building blocks for a larger system \cite{dynamic-field-theory-movement-preparation}, operating on a level of high abstraction from chemical processes.

The connectionist view is based on the proposition that ANNs as described in section \ref{sec:introduction} model the functionally relevant processes in biological brains in sufficient detail. Such systems have shown state-of-the-art performance on diverse tasks, in some cases even surpassing human performance \cite{multi-column-deep-networks-image-classification}. One important property that is required of machine learning models is the ability to \emph{generalize}. If given training data and the desired labels, a neural network should not merely learn to classify this specific data\footnote{This could be solved perfectly by just storing the data and the label}, but also learn to correctly classify new and unseen data, so long as it is reasonably similar to the training data, even if the specifics vary.

To probe a model's generalization ability, a part of the available data is usually withheld from training, effectively making it unseen data. After training, the networks performance on this set can be evaluated to obtain an estimate of its performance in practice. Even though training and test data may be a bit stronger correlated than the training data and the input in a productive environment\footnote{For instance due to changes in illumination or camera equipment}, ANNs have demonstrated reasonable generalization ability.

However, some discoveries have called this ability under scrutiny. A recent paper \cite{intriguing-properties-of-neural-networks} demonstrated that a state-of-the-art neural network can be tricked into misclassifying an input image by applying minuscule changes, even small enough to be imperceptible by humans. The thusly altered images are called \emph{adversarial examples (AEs)}. Their existance is problematic because from a generalization standpoint the minimum requirement would be that a network classifies visually indistinguishable images identically if it has a high confidence on one of them. However, network confidence is not an obstacle to finding AEs, they exist even for images where the network makes a certain and correct class affiliation assertion. Figure \ref{fig:intriguing-properties-ae} shows such an adversarial example\footnote{Even though \cite{intriguing-properties-of-neural-networks} contains a link to full-resolution images, it is dysfunctional as of the time of this writing (Error 404)}.

\begin{figure}[h!tb]
\centering
	\includegraphics[width=\textwidth]{images/intruiging_properties_ae.png}
	\caption[Adversarial example from \cite{intriguing-properties-of-neural-networks}]{Original image (left) and adversarial example (right, classified as \emph{ostrich, Struthio
camelus}) with magnified difference in the middle, as presented in \cite{intriguing-properties-of-neural-networks}}
	\label{fig:intriguing-properties-ae}
\end{figure}

It is apparent that the function represented by the network is undesirably discontinuous and lacking in smoothness, leading to strongly fragmented classification regions. Adversarial examples can exist as a result, they occupy regions proximal to training points in the data manifold, but on the other side of such a discontinuity. Naturally, the question arises how to find such perturbations given a (training or test) image. In \cite{intriguing-properties-of-neural-networks}, this is posed as an optimization problem according to equations \eqref{eq:ae-formalization-minimize} - \eqref{eq:ae-formalization-valid}.

\begin{align}
	\text{Minimize $\norm{\vec{r}}_2$ s.t.} \label{eq:ae-formalization-minimize} \\
	f(\vec{x} + \vec{r}) &= l \label{eq:ae-formalization-label} \\
	\vec{x} + \vec{r} &\in \brackets{0, 1}^m \label{eq:ae-formalization-valid}
\end{align}

Here, $\vec{x}$ is the given image, $\vec{r}$ its perturbation, $f$ the function embodied by the network, l the desired (wrong) label and $m$ the size of the flattened pixel vector. Approximate solutions to this problem were obtained for various images and target labels with an L-BFGS algorithm.

\subsection{Fast Gradient Sign Method}
\label{sec:fast-gradient-sign-method}
The major drawback of this approach is the required computation time to generate the adversarial examples. In order to study AEs and their properties in a quantative way and obtain significant statistical figures, this is insufficient. Fortunately, a much more efficient method was proposed in \cite{explaining-and-harnessing-adversarial-examples}. The fundamental idea is to optimize the input layer much like the parameters of the network, but with a different goal in mind. In order to understand the specifics of the process, consider the formalization\footnote{Some variable names have been changed to for consistency with equations \eqref{eq:ae-formalization-minimize} - \eqref{eq:ae-formalization-valid}} in equation \eqref{eq:fast-gradient-sign-method}.

\begin{align}
	\vec{r} &= \epsilon \sign \braces{\nabla_{\vec{x}} J\braces{\vec{\theta}, \vec{x}, l}} \label{eq:fast-gradient-sign-method}
\end{align}

Here, $J$ denotes the loss function, $\theta$ the parameters of the network, and $\epsilon$ a strength modulation parameter. $x$, $l$ and $r$ again stand for the original image, label and perturbation, respectively. In words, the formula takes the gradient of the loss function w.r.t. the input, applies the signum function and scales the result.

% TODO be more precise about what gets linearized
Using the gradient of $J$ w.r.t. $\vec{x}$ is easily motivated: If one choses an untrue $l$, this provides the direction in which the input should be changed in order to convince or dissuade the ANN to predict $l$ for this input, depending on whether $\vec{r}$ will be added to or subtracted from $\vec{x}$. The signum function is used to normalize the norm of the gradient. Assuming a linearization around $\vec{x}$, adding $\vec{r}$ corresponds to traversing the input space towards a prediction of label $l$, with $\epsilon$ denoting the distance traveled. In \cite{explaining-and-harnessing-adversarial-examples}, this is called the \emph{fast gradient sign method} (FGSM)\footnote{the shorthand is not part of the publication} for generating adversarial examples. An example for an AE generated with this method is shown in figure \ref{fig:harnessing-ae}.

% TODO discuss how the difference is much more like white noise compared to the L-BGFS one
\begin{figure}[h!tb]
	\centering
	\includegraphics[width=\textwidth]{images/harnessing_ae.png}
	\caption[Adversarial example created via FGSM, from \cite{explaining-and-harnessing-adversarial-examples}]{Original image (left) and adversarial example (right, classified as \emph{gibbon} with $99.3\%$ confidence) with magnified perturbation in the middle, as presented in \cite{explaining-and-harnessing-adversarial-examples}}
	\label{fig:harnessing-ae}
\end{figure}

Of course, linearization of the loss function (which includes the entire network) around $x$ is a rather strong assumption\footnote{Below we will discuss a linear explanation for the existence of adversarial examples, but it does not easily translate to a one-step ae generation algorithm that relies on linearity of the \emph{entire} network}. Rather than making one big leap in the input space, one can easily turn this approach into an iterative procedure of multiple small steps, in the manner of a proper gradient descent, as will be described in section \ref{sec:iterative-fgsm}. This way, one is able to find nearby AEs even if the linearity assumption is inapplicable, or gives a poor approximation.

Even though the FGSM is considerably more efficient compared to the L-BGFS method, repeatedly executing it over numerous iterations again poses the challenge of computation time. At this point it is crucial to realize that the gradient $\nabla_x J$ can be computed efficiently with backpropagation, just like the gradient w.r.t $\theta$. Conceptually, the input can be considered to be just another layer positioned below the first layer of an ANN, merely necessitating another application of the chain rule\footnote{Most neural network software by default won't compute the gradient for the input because it is usually unnecessary. Caffe provides a flag to trigger this computation}. For maximum efficiency, one could even try to integrate AE generation into regular backprop training, with the (possibly minor) complication that AE generation with multiple iterations would be performed in relation to a network with changing weights\footnote{this was not tested in this thesis}.

\subsection{Linear explanation of Adversarial Examples}
\label{subsec:ae-linear-explanation}
% TODO source for resistance against distortions and noise
The existence of adversarial examples naturally begs the question how they are even possible and what parts of a model they can be attributed to. They are a surprise since strict splits into training and test sets were believed to vouch for the generalization abilities of ANNs. Experiments with distorted and noisy input data point in the same direction. Since these methods fail to find significant and systematic flaws in neural networks, it is apparent that adversarial examples reside in narrow, low-probability regions of the data distribution, therefore requiring a directional instead of a randomized search.

Since AEs hint at precipitous discontinuities in the input-output map realized by the network, it is inviting to blame them on the nonlinear nature of neural networks. However, as pointed out by \cite{intriguing-properties-of-neural-networks}, even unequivocally linear models such as a softmax linear classifier are susceptible to adversarial examples. An explanation which doesn't require the assumption that neural networks heavily relying on their nonlinear powers was suggested in \cite{explaining-and-harnessing-adversarial-examples}.

This explanation focuses entirely on the linear components of an ANN and high-dimensional input data. Going back to the neuron model layed out in section \ref{sec:artificial-neural-networks}, equations \eqref{eq:neuron-math-membrane-potential} - \eqref{eq:neuron-math-activation-function}, the output of a single unit is computed according to

% TODO better variable names here as well
\begin{align}
	o &= \sigma \braces{\vec{w}^T \vec{i}}
\end{align}

where $\vec{w}$ is the weight vector, $\vec{i}$ an input vector of the same size, $\sigma$ the logistic sigmoid activation function. Lastly, $o$ is the output and the value we want to fudge. Note that $\vec{w}^T \vec{i}$ is simply an inner product, and therefore linear. If the inner product is large, $o$ will approach $1$, and if it reaches very far into the negative range it will approach $0$. Since $\vec{i}$ is the only variable we have control over, this is what we need to use to modify $o$.

% TODO just even $n$, not only powers of 2?
To simplify the scenario, binary vectors $\vec{i}, \vec{w} \in \brackets{0,1}^n$, where $n$ is a power of 2 are considered. This is not a technical necessity and the steps described below would work just the same\footnote{Safe for the coefficient introduced below which would need to scale with the size of the elements of $\vec{i}$}, but it allows for a less verbose description. Since the order of the elements of the vectors does not matter, a somewhat orderly weight vector is chosen where the first half contains $+1$ elements and the second half $-1$ elements, so $\vec{w} = \brackets{+1 +1 \dots -1 -1}$. Meanwhile, consider an input of alternating binaries, $\vec{i} = \brackets{+1, -1, +1, -1 \dots}$. In this case, $\vec{w}^T \vec{i} = 0$ holds for symmetry reasons, meaning the neuron is in an undecided state. There is nothing wrong with that, but to fully unfold the effect that is to be demonstrated, the last 4 elements of the input vector are always set to $+1$, leading to $\vec{w}^T \vec{i} = -4$ and subsequently $\sigma \braces{\vec{w}^T \vec{i}} \approx 0.0180$, independently of $n$. This is depicted in figure \ref{fig:ae-explanation} for various powers of $2$ as $n$ (blue crosses).

% TODO stack vertically for better readability
\begin{figure}[h!tb]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{ae_explanation_ip.png}
        \caption{Inner product of original and adversarial input}
        \label{fig:ae-explanation-inner-product}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{ae_explanation_output.png}
        \caption{Neuron output for original and adversarial input}
        \label{fig:ae-explanation-neuron-output}
    \end{subfigure}
    \caption{Inner product and neuron output depending on data dimensionality}
    \label{fig:ae-explanation}
\end{figure}

% TODO add a table that shows the same data as the image

For the given input, the neuron is firmly in the off-state. In order to change it to the on-state, the inner product needs to be increased and move into the positive region. Simply put, the inner product will be positive when the input is positive where the weight vector is positive, and negative where the weight vector is negative. Indeed, one could simply set $\vec{i} \coloneqq \sign \braces{\vec{w}}$ since this always fulfills the requirement\footnote{The $\sign$ function is only given to sustain the universality of the approach. Since $\vec{w}$ is a binary vector here, it serves no purpose in this specific context.}. However, that is not a helpful approach when trying to create an adversarial example for the input, since $\vec{i}$ might be vastly different from $\sign \braces{\vec{w}}$. But it is a viable option to change $\vec{i}$ slightly so that it becomes more similar to the sign of the weights. This is described by equation \eqref{eq:ae-explanation-formula}.

% TODO maybe use c instead of \epsilon in eq:fast-gradient-sign-method. It should be the same letter and c is much better.
\begin{align}
	\vec{\hat i}&= \vec{i} + c \cdot \sign \braces{\vec{w}} \label{eq:ae-explanation-formula}
\end{align}

% Note that all of this is similar to the FGSM, but only tries to trick one neuron instead of the network output, and therefore operates on the weight vector instead of the gradient. Can be considered a crude variant of FGSM.
Here, $\vec{\hat i}$ denotes an adversarial example and $c$ is a strength modulation coefficient. It is not surprising that it resembles equation \eqref{eq:fast-gradient-sign-method} for the FGSM, since the goals are similar. But while FGSM tries to trick an entire network, this equation merely tries to change the output of a single neuron in order to demonstrate the susceptibility of (partly) linear models to adversarial perturbation. Since it does not need to take into account how the neuron's output is processed in higher layers of the network, it doesn't operate on the gradient but simply on the weight vector.

The effect of this perturbation is also shown in figure \ref{fig:ae-explanation} for a value of $c = 0.001$ (green crosses). As \ref{fig:ae-explanation-inner-product} demonstrates, the strength of the perturbation's effect on the inner product increases with $n$. It is not surprising that a change of constant size to each element amounts to more the larger the vector is, but what is surprising is the strength of this effect. As figure \ref{fig:ae-explanation-neuron-output} indicates, the critical point where the perturbation forces the neuron into the on-state is located around $n = 4096$.

However, when examining both \ref{fig:ae-explanation-inner-product} and \ref{fig:ae-explanation-neuron-output}, one can see that there is a significant visual discrepancy. The critical point around $4096$ is still in a region where the curve for the inner product is still very flat. This gives reason to believe that with a slight increase in data dimensionality, the method can find adversarial examples even for such input data that leads to a much more confident off-state decision (remember that the original inner product was merely $-4$). It is also worth considering just how small of a change $c = 0.001$ implies. Since the vectors are binary, a change of $0.001$ per element means that each element's value gets changed by only $\frac{1}{1000}$ of its scale. This is a trade-off between dimensionality and $c$, larger input allows for a smaller $c$ while smaller input requires a larger $c$.

In conclusion, one does not need to invoke nonlinear properties of neural networks to explain the existence of adversarial examples. Indeed, it is preferable to not do so, since while a neural network is \emph{potentially} nonlinear owing to their activation functions, most of these\footnote{and, in fact all of those shown in figure \ref{fig:activation-functions}} contain a region that at least approximates a linear function. Thus, basing an explanation on nonlinearity makes some additional assumptions about the distribution of a networks membrane potentials. Instead of that, the linear explanation relies only on properties of the inner product and input data dimensionality. It was demonstrated that it becomes easier and easier to tamper with the value of an inner product, and hence with neuron outputs, as the data size grows, while keeping the changes to each individual value in the input minimal.

% TODO finish this section
\subsubsection{Adversarial Examples in Convolutional Neural Networks}
The explanation given in subsection \ref{subsec:ae-linear-explanation} relates adversarial examples mostly with inner products in high-dimensional spaces. But even though convolutional neural networks are just as susceptible to adversarial examples, their first operation on the input data is usually a convolution which typically uses relatively small filters, in the order of $3 \times 3$ to $7 \times 7$. While it is possible to chose a larger coefficient to have a greater leverage on the the value of the inner product, a reflection on the nature of CNNs allows for a different answer.

% TODO finish this
A crucial point is that the explanation given addresses a single neuron, but a convolutional network consists of many layers, each of which contains numerous neurons. Ultimately, the success of an adversarial example depends on manipulating the values in the last layer, not the first. So while it might not be possible to have a strong impact on individual neurons in the first convolutional layer, small changes at many of its neurons will be aggregated by subsequent layers\footnote{Note that while this explanation does require more than 1 hidden layer, it does not rely on the nonlinearity between them.}. Thus, in a sense, a CNN also realizes high-dimensional inner products, but defers their computation to higher layers.

\subsection{Universality of Adversarial Examples}
% TODO source for MNIST
% TODO figure out what an autoencoder is
In \cite{intriguing-properties-of-neural-networks}, two more concerning properties of adversarial examples were exposed. Both indicate that AEs are notably more universal than one might expect. The first relates to generalization across multiple models. In the publication, 6 diverse models were trained on the \emph{MNIST} data set, comprising 3 linear softmax classifiers, 2 ANNs with hidden layers and 1 autoencoder. Adversarial examples generated for each of these were evaluated on the other models. The results show that the AEs generalized surprisingly well to other models\footnote{The autoencoder was more resistant to foreign AEs, but still notably more prone to being tricked by them than by random noise on the input data.}.

The other observed property is a universality across different training sets. Neural networks with 2 hidden layers were trained on disjoint, equally large portions of the MNIST training set. Analogously to the above, adversarial examples were generated for each ANN and used for testing on the others. Although they did not retain as much of their deceptive power as they did in the cross-model test, the misclassification rate on the same model trained on seperate data is still considerably higher than both the error on the unmodified test set and random noise.

This universality of AEs is not only astounding, but also problematic since it forecloses a simple solution to the problem. If adversarial examples were model specific, a committee of different or differently trained models could be used to increase resistance against AEs by outvoting the affected model.

\subsection{Relating the Linear Explanation to the Network Function}
% TODO "is still an open question" as far as I'm aware
Since adversarial examples are able to generalize across both different hyperparameters and different training data choices, their existence seems to be more like a fundamental property of the data space than a noise-like artifact of a specific network or of the choice of samples from the data distribution. In \cite{explaining-and-harnessing-adversarial-examples} it has already been established that AEs exhibit some directional contiguity. How this ties in with the topology learned by the network is still an open question. One possible perspective on this matter is attained by considering the function represented by an ANN as a homeomorphism, which is at least valid for nonsingular weight matrices. In that case, the network is only able to stretch and deform the input space as shown in figure \ref{fig:network-homeomorphism} before making a decision, but it can't cut and sew up the space.

\begin{figure}[h!tb]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{homeomorphism_start.png}
        \caption{The untransformed input space.}
        \label{fig:network-homeomorphism-start}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{homeomorphism_end_1.png}
        \caption{Space as transformed by the network.}
        \label{fig:network-homeomorphism-end}
    \end{subfigure}
    \caption[Network function as homeomorphic transformation of input space]{Illustration of a homeomorphic transformation of the input space in a binary classification task. Blue and red areas indicate the data distribution and the black line represents the decision boundary. Images taken from \cite{neural-network-homeomorphics-transformation}}
    \label{fig:network-homeomorphism}
\end{figure}

Because the chosen network is not powerful enough to disentangle the classes, it minimizes the average error by elongating the space such that the entangled part becomes minimal. As a consequence the data manifolds for both classes are highly deformed and stretched. A result from \cite{explaining-and-harnessing-adversarial-examples} is that the contiguous regions of adversarial examples radiate outward from the original image. However, since randomized alterations of the input image hardly ever produce adversarial examples, it is clear that if they radiate outwards, they must do so in very thin\footnote{albeit long, and therefore possibly voluminous} strips. The homeomorphic perspective provides a topological reason for the network to do this. Since it can't really \emph{solve} the problem because the data manifolds are too strongly entangled, it minimizes the error by deforming the regions classified erroneously into low probability regions.

% TODO
%% What are they
%% How are they created (two ways)
%% Why is their existence surprising?
%% Deviation from the data manifold?
%% Why do they exist? (strongly fragmented classification regions outside data manifold, lacking smoothness)
%% Linear explanation for AE's existence
%% Generalization across different models and subsets of the data used in training
% They tried to train with AEs increase network's resistance, didn't work (but improved generalization, even more than dropout does)
% Theoretical ramifications for human vision
%% How does the explanation given in "Harnessing AEs" relate to CNNs?
%% Relate existence of AEs with homeomorphism-likeness of ANNs. Harnessing AEs section 8 gives some support
%% Points from the summary of Harnessing Adversarial Examples
% How do _I_ create them? The thing with the iterations-dependent coefficient...








\section{Iterative Fast Gradient Sign Method}
\label{sec:iterative-fgsm}
% TODO FIXME expand this section
In order for large-scale quantitative analysis of the properties of adversarial images to be feasible, a method for efficiently generating adversarial examples is expedient. Apart from a reasonable use of computation time, such a method should also produce AEs with reliable quality, meaning that it should not try to fool a model by sometimes making clearly visible changes to the original data. This would necessitate visual inspection of all created AEs. Ideally, the maximal perturbation performed by the algorithm is knowable beforehand so one can be sure that all generated examples which manage to trick the network are actually adversarial. This section presents an algorithm that fulfills these two criteria.

% FIXME references
This algorithm will be used both to create the individual adversarial examples of section \ref{sec:fundamental-properties} and the large amount of AEs described in section \ref{sec:generation-large-sets}, which serve as a basis for the analyses in sections \ref{sec:spectra}, \ref{sec:class-dependence-generation-success} and \ref{sec:predicting-final-confidence}.

At this point it is vital to lay out in detail how the adversarial examples inspected and used in this thesis are generated. The process is based on the Fast Gradient Sign Method introduced in section \ref{sec:fast-gradient-sign-method} and detailed in equation \eqref{eq:fast-gradient-sign-method}. Briefly stated, the gradient of the loss function w.r.t. the input data is used to determine the direction in input space along which the data should be altered. Of course, in order to generate AEs and trick the networks into predicting a false class, a false label is used in the loss function. This approach lends itself to an iterative algorithm, formalized as pseudocode in algorithm \ref{alg:make-adversarial}.

% TODO grad[0] indicates gradient for input
\newcommand\numIt{\text{num\_iterations}}
\newcommand\advEx{\text{ae}}
\newcommand\grad{\text{grad}}
\begin{algorithm}
	\begin{algorithmic}
		\Function{IterativeFGSM}{input\_data, l, c, \numIt}
			\State \advEx $\ \gets$ input\_data
			\State $i \gets 0$
			\While{$i < \numIt$}
				\State $\grad \gets$ \Call{Network.Backprop}{\advEx, l}
				\State $r \gets \frac{c}{\numIt} \cdot \sign \braces{\grad[0]}$
				\State \advEx $\ \gets$ $\advEx + r$
				\State $i \gets i + 1$
			\EndWhile
			\State \Return \advEx
		\EndFunction
	\end{algorithmic}
	\caption[Iterative FGSM]{Generate adversarial examples with iterative FGSM}
	\label{alg:make-adversarial}
\end{algorithm}

This approach is based on \cite{how-to-trick-neural-network-panda-to-vulture} and relies on an iterative strengthening of an image's adversarial properties. It is described in a data-agnostic manner, even though it is only used for images in the scope of this thesis. The AE gets initialized with the image data. Then a selectable number of iterations is performed where as a first step, the gradient is computed via backpropagation. Subsequently, the gradient of the input data is extracted\footnote{In the pseudo-code this is indicated by \emph{grad[0]}, but the specifics will depend on the software and libraries used} and its sign is computed, in keeping with FGSM. This signed gradient is then modulated by a coefficient and added to the adversarial data. This coefficient, however, slightly deviates from the description in \eqref{eq:fast-gradient-sign-method}. Perturbation $r$ is formed by modulating the gradient with $\frac{c}{\numIt}$, meaning that the adaption strength coefficient $c$ is scaled with the number of iterations. It is functionally equivalent with simply choosing a smaller $c$, but provides the advantage that the parameter scales are comparable to the single-step approach of \cite{explaining-and-harnessing-adversarial-examples}. There, a value of $0.9$ means a distance of $0.9 \cdot \norm{r}$ traveled in input space, and here, it indicates the same distance traveled, although the path may be curved so the distance between start and end point may be smaller. Thus, a value of $c = 0.9$ describes a sphere of radius $0.9 \cdot \norm{r}$ around the starting location, containing all eligible end points for the AE search. Besides comparability, this geometric meaning is another reason to link step size and iteration count.











\section{Analysis of fundamental properties}
\label{sec:fundamental-properties}
With the method presented in section \ref{sec:iterative-fgsm}, it is possible to create convincing adversarial examples that both trick the network into predicting a false class, and are indistinguishable from regular images by humans at the same time. How well it works depends on the original image, target class and careful choice of the parameters and will be analyzed in detail further below, but for now a functional example is given in figure \ref{fig:ae-example}.

\begin{figure}[h!tb]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{aes_new/panda.png}
        \caption{Original, rescaled image as fed into the network. Top prediction: \emph{giant panda}, 99.23\% confidence}
        \label{fig:ae-example-original}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{aes_new/panda_acorn_10_0dot9.png}
        \caption{Adversarial example. Top prediction: \emph{acorn}, 86.17\% confidence \\}
        \label{fig:ae-example-ae}
    \end{subfigure}
    \caption[Adversarial example of a panda as an acorn, generated with iterative FGSM]{An image of a panda and its adversarial form. The latter was generated with $c = 0.9$ and 10 iterations on ImageNet/GoogLeNet}
    \label{fig:ae-example}
\end{figure}

% TODO table with predictions
% TODO with 100 iterations, 94.49% is possible -> elaborate on this (with pictures)
The original image is of a giant panda facing upwards, taken from above. The file has a resolution of $1728 \times 1152$ pixels. However, the adversarial example for this image was created by using the GoogLeNet CNN, which accepts input only in $224 \times 224$ resolution, so the image was rescaled to match. In \ref{fig:ae-example-ae} one can see an adversarial example for the panda, which, although no changes are visible to human eyes, is classified as an acorn by the network with an inappropriately high confidence of $86.17\%$. With 100 iterations, $94.49\%$ are possible while keeping $c = 0.9$, adding even more unjustified confidence to a wrong prediction at the expense of ten times as much computational time\footnote{For this reason, the large-scale production of AEs that will be described below will use many more iterations than just 10 or 100.}.

Even though no visible changes are apparent, the right image is clearly not identical to the one on the left, hence the misclassification. In order to see exactly how they deviate from each other in image space, one can look at their pointwise differences. These are depicted in figure \ref{fig:ae-difference-diff} below.

\begin{figure}[h!tb]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{aes_new/panda_acorn_10_0dot9_posdiff.png}
        \caption{Pointwise difference of original and adversarial image, positive part. See appendix \ref{sec:visualizing-diff} for details.} % TODO
        \label{fig:ae-difference-diff}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{aes_new/panda_acorn_10_0dot9_grad.png}
        \caption{Sign of the gradient in the first iteration as used by iterative FGSM. Scaled by the 98th percentile to scale to visible range}
        \label{fig:ae-difference-grad}
    \end{subfigure}
    \caption[Difference between original and adversarial example]{Illustration of the deviation between the images in figure \ref{fig:ae-example}.}
    \label{fig:ae-difference}
\end{figure}

As one can see, the method devotes its effort into regions that are meaningfully seperated as far as humans are concerned. Apparently, a lot of the changes that are determined to be impactful are made at the panda's eyes and their immediate surroundings, corresponding to the black spots they tend to have. This allows for the conclusion that ImageNet attaches a lot of importance to these features when trying to detect a panda. Naturally, if one wants to convince the network that it is an acorn, it is not only expedient to emphasize acorn-like features, but also to attenuate panda-like features, explaining the changes above. Both the nose and snout are also discernable, and, to a lesser extend, the ear on the right side\footnote{which is the panda's left ear}. Although the intensity of the change seems to be weaker compared to the eye region, they too seem to have a significant bearing on the network's impression of a panda.

% TODO image to justify that panda head shape looks like acorn
Another circumstance uncovered by figure \ref{fig:ae-difference-diff} is the emphasis on the outline of the panda. This concurs with the notion that the geometry of objects is of considerable importance and acts as a reliable determinant of class membership. Because of this, it is advisable for the algorithm to focus on contours to maximize its effectiveness. Yet, if this explanation is to hold some ground, the question arises why one can only see changes to the existing panda's outline, but not see a new acorn's outline emerge. There are two possible explanations for this: Either the adversarial example convinces the network of its acorn-ness via features other than overall shape, or it modifies and reuses part of the panda's shape to look more like an acorn. The former explanation can draw some support from the diffuse, wide-spread changes outside the panda in figure \ref{fig:ae-difference-diff}. On a speculative note, they might serve to fudge the texture and other high-frequency features. The latter hypothesis is mostly founded in the observation that the panda's head, and specifically its outline highlighted by the difference of original and AE, roughly resembles an acorn's shape. A hand-picked example to justify this line of reasoning is given in figure \ref{fig:imagenet-acorn-dresser-acorn}.

\begin{figure}[h!tb]
    \centering
    \begin{subfigure}[b]{0.37\textwidth}
    		\centering
        \includegraphics[scale=0.5]{imagenet/n12267677_6653.JPEG}
        \caption{An acorn (n12267677\_6653.jpeg)}
        \label{fig:imagenet-acorn-dresser-acorn}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.53\textwidth}
        \includegraphics[width=\textwidth]{imagenet/n03016953_140.JPEG}
        \caption{A dresser (n03016953\_140.jpeg)}
        \label{fig:imagenet-acorn-dresser-dresser}
    \end{subfigure}
    \caption[An acorn and a dresser from ImageNet]{Representatives for acorns and dressers in ImageNet}
    \label{fig:imagenet-acorn-dresser}
\end{figure}

To test this hypothesis, the same procedure with the same parameters was repeated, with the only difference being the target class. Instead of an acorn, the iterative FGSM was instructed to turn the panda into an instance of the dresser class, with a class representative given in figure \ref{fig:imagenet-acorn-dresser-dresser}. The images of dressers exhibit angular features, with the rectangular main body being segmented into likewise rectangular compartments. A strong attempt to recreate a similar edge structure in the AE should be clearly visible. In figure \ref{fig:ae-dresser-difference-grad}, the resulting difference is depicted\footnote{The resulting image is visually indistinguishable from figure \ref{fig:ae-example} and therefore omitted.}.

\begin{figure}[h!tb]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{aes_new/panda_dresser_10_0dot9_posdiff.png}
        \caption{Pointwise difference of original and adversarial image}
        \label{fig:ae-dresser-difference-diff}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{aes_new/panda_dresser_10_0dot9_grad.png}
        \caption{Sign of the gradient in the first iteration as used by iterative FGSM}
        \label{fig:ae-dresser-difference-grad}
    \end{subfigure}
    \caption[Difference between original and a failed adversarial example]{Illustration of the deviation between the panda in figure \ref{fig:ae-example-original} and an adversarial example mimiking the class represented by \ref{fig:imagenet-acorn-dresser-dresser}. Corresponding AE was generated in $10$ iterations with $c = 0.9$.}
    \label{fig:ae-dresser-difference}
\end{figure}

% TODO table for predicitons
As one can see, figure \ref{fig:ae-difference-diff} and \ref{fig:ae-dresser-difference-diff} are overwhelmingly similar, only differing in the noise-like pattern in which the individual changes are locally arranged. Rectangular shapes, much less a dresser, are not visible, which makes the hypothesis seem less likely to be true. However, a noteworthy circumstance is that in this case, the algorithm did not succeed in turning the panda into a dresser. The top prediction was \emph{n02120079, Arctic fox} with $8.96\%$ certainty, with \emph{n02510455, giant panda} still in 5th place with $2.77\%$. The class \emph{n03016953, dresser} was not even in the top 5 predictions. This is not unique to the dresser class, rather, it seems the panda is rather intractable when trying to turn it into rigid, angular objects\footnote{though only a small sample of eligible target classes was tested}.

% TODO try c=3.0 and 500 iterations for this
As the case stands, it is possible that feigning a more dresser-like outline would be effective, but can not be performed by the algorithm in this configuration. Thus, instead of choosing a target class that suggests a more visible output, an attempt was made to elicit a more evident acorn shape in the difference by increasing the algorithms power to modify the image, even if it comes at the expense of the produced image's adversariality. This was done by increasing the value of $c$ to a an excessive $3.0$. The resulting difference between original and adversarial image is shown in figure \ref{fig:ae-high-power}.

\begin{figure}[h!tb]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{aes_new/panda_acorn_10_3dot0_posdiff.png}
        \caption{Pointwise difference of original and adversarial image}
        \label{fig:ae-high-power-img}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{aes_new/panda_acorn_10_3dot0_grad.png}
        \caption{Sign of the gradient in the first iteration as used by iterative FGSM}
        \label{fig:ae-high-power-diff}
    \end{subfigure}
    \caption[Difference between original and excessive adversarial example]{Illustration of the deviation between the panda in figure \ref{fig:ae-example-original} and an adversarial example mimiking the class represented by \ref{fig:imagenet-acorn-dresser-acorn}. Corresponding AE was generated in $10$ iterations with $c = 3.0$.}
    \label{fig:ae-high-power}
\end{figure}

Here, the visual similarities to something like \ref{fig:imagenet-acorn-dresser-acorn} are considerably more pronounced, so it may actually be the case that the iterative FGSM operates on the level of shapes and contours in order to convince the network that an image has a certain class. However, one has to be aware that a sizable portion of the similarity is also promoted by the initial round shape of the panda's head. Note that this is highly speculative and requires further investigation before any definitive conclusions can be drawn.

Also, while outlines are evidently important, it would be a false conclusion to assume that FGSM operates \emph{exclusively} on outlines. Even though some clues have been presented that indicate a significant role of larger shapes, it is crucial to keep in mind that other factors may contribute important success factors as well, such as texture or a completely delocalized noisy effect, which are not ideally covered by displaying the most prominent changes in the image.

Another interesting aspect is the relationship between difference and gradient. Figures \ref{fig:ae-difference}, \ref{fig:ae-dresser-difference} and \ref{fig:ae-high-power} not only show the difference between original image and the completed adversarial example, but also the gradient at the location of the original image\footnote{This is the gradient used in the first iteration}. One aspect deserves attention when contrasting these two: the distribution of values across the image is vastly dissimilar. Normally, this would be expected since the merit of an iterative process like a gradient descent is its ability to adjust its course based on local information of the optimized function at the current location. Therefore, the point where the process terminates is not usually aligned with the direction of the first step\footnote{If this were expected to happen, then one could have simply done a binary search on the step size and reach the same point (or an even better point) faster}.

However, in this case it is still worth discussing, because in \cite{explaining-and-harnessing-adversarial-examples} it was discovered that adversarial examples can be found in this initial direction, given an appropriate step size is used. However, here we find that the direction in input space has clearly changed over the course of the optimization, and above we have also noted that performing $100$ instead of $10$ iterations can boost performance, even when the maximum distance possibly traversed in input space is the same.

These two results are not strictly mutually exclusive; the existence of adversarial examples in the initial direction does not prohibit the existence of other, and possibly better, AEs in different directions. Also the conclusion from \cite{explaining-and-harnessing-adversarial-examples} that cross-model generalization of AEs is caused by an easily learned, essentially linear structure in input space still holds, as long as the initial direction and the path taken by gradient descent are roughly aligned. However, the findings above cast the relation between input space and adversarial examples in a whole new light and hint at a finer structure in this space that might exist as low-amplitude, possibly noise-like ripples in the generally linear structure.

\subsection{The effects of iterations and adaption strength coefficient}
% TODO FIXME 94.49 fix discrepancy with table
The previous section revealed a relationship between iteration count and the effectiveness of the generated adversarial example. Running iterative FGSM for $100$ instead of $10$ iterations yielded a GoogLeNet confidence of $94.49\%$ instead of $86.17\%$. In order to be able to efficiently generate large batches of adversarial examples, an understanding of this relation is indispensable. To this end, a plot relating these two quantities is provided in figure \ref{fig:confidence-vs-iterations}, accompanied by table \ref{tab:confidence-vs-iterations}, showing the exact values for the first 8 iteration counts.

% TODO better plot with 2 scales
% TODO more classes
\begin{figure}[h!tb]
	\centering
	\includegraphics[width=0.6\textwidth]{images/confidence-vs-iterations.png}
	\caption[Final network confidence w.r.t used iterations]{Plot of the network confidence at the end of AE generation as a function of iteration count. A value of $c = 0.9$ was used.}
	\label{fig:confidence-vs-iterations}
\end{figure}

\begin{table}[h!tb]
	\begin{tabular}{|l|llllllll|}
		\hline
		Iteration & 10 & 25 & 50 & 75 & 100 & 125 & 150 & 175 \\
		\hline
		Acorn confidence & 0.8613 & 0.9258 & 0.9392 & 0.9428 & 0.9456 & 0.9463 & 0.9469 & 0.9475 \\
		Dresser confidence & 0.0114 & 0.0158 & 0.0177 & 0.0183 & 0.01868 & 0.01899 & 0.0191 & 0.0191 \\
		\hline
	\end{tabular}
	\caption[Final network confidence w.r.t iterations]{Table of the data (rounded to 4 figures) corresponding to figure \ref{fig:confidence-vs-iterations}}
	\label{tab:confidence-vs-iterations}
\end{table}

The original image was again the panda from figure \ref{fig:ae-example-original}. Since acorns and dressers behaved very differently as target classes, both were tried for iterations varying from 10 to 500. For the acorn, the first two data points show the strongest increase in network confidence, by a little under $6.5\%$. Afterwards, the improvements quickly taper off and the network predictions start to saturate at slightly above $94\%$. This demonstrates that there exist pairs of images and target classes for which the fast gradient sign method stabilizes at a value below 1, even if ample computation time is provided.

In contrast to the transformation toward an acorn prediction, the attempt to provoke a prediction as a dresser seems much less promising. While the former did not saturate around 1, it at least saturated around a high confidence value, being the clear top prediction of the network. The presumed dresser, though, hardly makes any progress. It attains a mere $1.1\%$ confidence with 10 iterations and only manages to achieve slightly under $2\%$ even if given 500 iterations. While this observation is not sufficient to conclude that turning a panda into an adversarial example of the dresser class is impossible with iterative FGSM\footnote{It might still be possible with a higher change coefficient, which will be analyzed below}, it clearly shows that for a given ANN, and thus fixed input dimension, a specific value for $c$ may work for one target class but not the other, even if the original image is identical. It is a necessary conclusion that the way in which adversarial examples are structured in the input space is not symmetrical w.r.t the target class.

Another parameter whose impact on the success of adversarial example generation is worth exploring is the adaption strength coefficient $c$. The choice for this value has to strike a balance between the algorithm's capability to modify the original image in order to trick the model, and limiting the distance traveled from the original image to ensure that the resulting AE will be visually indistinguishable from it. To gain some insight into this relation, GoogLeNet confidence on multiple adversarial target classes as a function of $c$ has been plotted in figure \ref{fig:confidence-vs-coeff}, while the precise values are given in table \ref{tab:confidence-vs-coeff}.

\begin{figure}[h!tb]
	\centering
	\includegraphics[width=0.6\textwidth]{images/confidence-vs-coeff.png}
	\caption[Final network confidence w.r.t adaption strength coefficient]{Plot of the network confidence at the end of AE generation as a function of adaption strength $c$. 100 iterations were performed.}
	\label{fig:confidence-vs-coeff}
\end{figure}

\begin{table}[h!tb]
	\begin{tabular}{|l|llllllll|}
		\hline
		c & 0.3 & 0.6 & 0.9 & 1.2 & 1.5 & 1.8 & 2.1 & 2.4 \\
		\hline
		Acorn confidence & 0.0015 & 0.2247 & 0.9456 & 0.9981 & 0.9999 & 1.0 & 1.0 & 1.0 \\
		Dresser confidence & 0.0001 & 0.0022 & 0.0187 & 0.1004 & 0.3051 & 0.6056 & 0.7792 & 0.9476 \\
		\hline
	\end{tabular}
	\caption[Final network confidence w.r.t adaption strength coefficient]{Table of the data (rounded to 4 figures) corresponding to figure \ref{fig:confidence-vs-coeff}}
	\label{tab:confidence-vs-coeff}
\end{table}

The graph shows that a value of $c = 0.9$, which was used for almost all AEs in this section, is a reasonable choice for the acorn target class. Lower values like 0.6 might still mislead the network to predict an acorn as the most likely class, but with significantly lower confidence. Slightly higher values manage to boost confidence to $100\%$, at which point the curve saturates. One can also observe that with a significantly higher $c$, it is possible to convince the CNN that the panda is a dresser. The curve for the dresser is of sigmoidal shape, starting its slope round $c = 0.9$, and saturating with a confidence level of $1.0$ near the end of its sloped segment around $c = 2.4$. These values are substantially higher than what could be achieved by increasing the number of iterations. This indicates that no adversarial examples for the dresser class exist as close to the original image as they do for the acorn class, but one might be able to find them further outward.

Yet, it just shows that GoogLeNet can be led to believe that the input shows a dresser. For the modified image to be an \emph{adversarial example}, it still needs to retain its appearance as a panda. Therefore, it is appropriate to inspect the generated images before appraising the success of this process. Since the confidence at $c = 1.8$ is the first one above $50\%$ and thus sure to make the network predict a dresser, the corresponding AE is shown in figure \ref{fig:ae-panda-dresser-1dot8}.

\begin{figure}[h!tb]
	\centering
	\includegraphics[scale=0.90]{images/aes_new/panda_dresser_100_1dot8.png}
	\caption[Flawed adversarial example obtained with high adpation strength]{Result of iterative FGSM on \ref{fig:ae-example-original} with $c = 1.8$, $100$ iterations and \emph{n03016953, dresser} as target class.}
	\label{fig:ae-panda-dresser-1dot8}
\end{figure}

% TODO fix reference to sec:spectra
% TODO find source for claim that deep NNs operate like human vision
Here, one can clearly see the visual artifacts that arose from modification of the original image. The image features a high concentration of intense color spots that follow a noise-like distribution around the panda's forehead. One can also observe a similar, but more sparse pattern on the left side\footnote{the beholder's left} of the panda's nose. These spots are high-amplitude, high-frequency features which led to an analysis of the frequency space of adversarial examples in section \ref{sec:spectra}. The impact of this circumstance is uncertain. On the one hand, it is easy to tell the supposed adversarial example apart from the original image, or any other regular image, making it somewhat not adversarial. On the other hand, the picture clearly shows what is still a panda, and one would ask of a CNN to be able to correctly identify it as such. While this finding doesn't disprove the ambitious claim that deep neural networks process images in a similar fashion to human brains, it highlights that even if they do, they can be effectively distracted and confused by localized, nonsensical information.

%Since figure \ref{fig:ae-panda-dresser-1dot8} revealed that the changes made by FGSM seem to be concentrated at one or very few regions, it is worth investigating whether that pattern holds for other combinations of original and target class. For the purpose of visual inspection,

\subsection{Iterative properties of the transformation progress}
\label{subsec:iterative-properties}
Even though iterative FGSM utilizes the same fundamental idea for generating adversarial examples as its non-iterative forerunner, it constitutes a change of paradigm in the algorithm. It is therefore advisable to examine the properties of the iterative nature of this new variant in more detail. Two facets of this nature have already been inspected in previous parts of this section: The relationship between the changes in the adversarial image and the first gradient, and how the final confidence of the network depends on the number of steps under a fixed value for $c$. One thing not yet analyzed is the development of the CNN's confidence over the course of these steps. Hence, two plots for $10$ and $100$ iterations respectively are provided in figure \ref{fig:confidence-evolution}.

% TODO 300 iterations
% TODO curve for a failed transformation, i.e. panda->dresser
% TODO there should not be a value for iteration 0
\begin{figure}[h!tb]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{confidence-evolution-10.png}
        \caption{10 iterations}
        \label{fig:confidence-evolution-10}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{confidence-evolution-100.png}
        \caption{100 iterations}
        \label{fig:confidence-evolution-100}
    \end{subfigure}
    \caption[Development of network confidence w.r.t iterations]{Development of network confidence over the iterations. Panda turned into an AE of class \emph{n12267677, acorn}, $c = 0.9$}
    \label{fig:confidence-evolution}
\end{figure}

Again, a panda has been transformed into what the network considers to be an acorn. Even though \ref{fig:confidence-evolution-100} has $10$ times as many iterations as \ref{fig:confidence-evolution-10}, the plots look strikingly similar. This is not entirely unexpected, though. Remember that the algorithm links step size and iteration count in the adaption strength coefficient together, so increasing iterations while keeping $c$ constant leads to a smaller step size and vice-versa\footnote{Reasons why this link may be desireable are given in section \ref{sec:iterative-fgsm}}.

% TODO fix reference sec:predicting-generation-process
% TODO elaborate more on the fact that some confidence goes to other classes.
Initially, the network confidently predicts the correct class. During the initial iterations, the confidence for this class drops, while the confidence of the adversarial class stays practically at $0$. Only later, at about the halfway point in the optimization process, does its confidence begin to rise until it reaches its maximum with the last iteration. The temporal delay between these two developments is surprising. GoogLeNet uses a final softmax layer, so all class probabilities sum up to $1$. This means that some of the confidence must be going to other classes not directly involved this process. The overall shape of the confidence curve is sigmoidal (although this is hard to see with 10 iterations). This observation serves as a basis for an attempt to predict the success of an AE generation process in section \ref{sec:predicting-final-confidence}

% TODO some images to illustrate the unknown classes teddy, hamster and custard apple
For the 100 iteration run, the minimal combined probability for true and target class is reached after iteration $53$. To see what the network thinks of the adversarial-image to-be at this point, the top 5 predictions alongside their corresponding probabilities are summarized in table \ref{tab:ae-intermediate}. The remaining probabilities appear to go to related classes, the panda and teddy classes for instance share bear-ness, and it is also not a long shot to the hamster, which is also furry and has a comparable body shape with a voluminous torso and short extremities. A custard apple, on the other hand, has convincing similarities to the top part of an acorn.

\begin{table}[h!tb]
	\centering
	\begin{tabular}{|l|l|}
		\hline
		Class & Probability \\
		\hline
		n02510455, giant panda & 0.1064 \\
		n04399382, teddy & 0.0929 \\
		n02342885, hamster & 0.0433 \\
		n12267677, acorn & 0.0371 \\
		n07760859, custard apple & 0.0364 \\
		\hline
	\end{tabular}
	\caption[Top 5 class probabilities in the middle of iterative FGSM]{Top 5 class probabilities after 53 iterations. Rounded to 4 figures}
	\label{tab:ae-intermediate}
\end{table}

A tempting fallacy would be to look at figure \ref{fig:confidence-evolution-10}, notice the distinctly non-zero slope at the end for the acorn class and conclude that with 10 iterations, the algorithm has been prematurely terminated. While it is true that more iterations improve the final prediction in the panda-acorn combination (cf. table \ref{tab:confidence-vs-iterations}), the reasons why this happens are a bit more intricate. In a lot of iterative algorithms, performing an additional step has no bearing on the previous iterations. However, since algorithm \ref{alg:make-adversarial} couples iterations and step size, choosing 11 iterations would not simply lead to an additional step, but lead to more, smaller steps. Thus, it is really the path taken by the algorithm that improves if more iterations are provided, not the distance traveled\footnote{Though one could in theory also just perform an additional step of size $\frac{c}{10}$ and most probably gain a higher confidence as a result.}. This is a superior approach, since one stays closer to the original image, strengthening the adversarial properties of the output.

% TODO
% What happens if we make a panda less / more like a panda?
% AE by superposition
% Plot extreme AEs, generated with very high c
% How does this compare to panda -> acorn with 1 iteration?








\section{Generation of large AE sets}
\label{sec:generation-large-sets}
% TODO I concluded that one coefficient doesn't work well for a given network / input size, or even one specific image when trying to turn it into multiple adversarial classes. Still, batch generation of AEs was done with one coefficient for all. Explain this with computation time restraints.

% TODO we focus on images
In order to gain insights into the nature and properties of adversarial examples, it is expedient to have a great number of them at one's disposal. This allows for a more automated, larger-scale analysis than visual inspection of AEs\footnote{though visual inspection is also a useful method and is employed in this thesis} and can unveil more general characteristics. To this end, multiple large-scale batches of adversarial examples were created based on different data sets and various configurations of hyper-parameters with the iterative FGSM algorithm detailed in section \ref{sec:iterative-fgsm}. 

% TODO almost all? Not just all?
% FIXME What about COIL 100? Has it been added?
Apart from parameters choices, the biggest determinants of the specifics of adversarial examples are the data set used to provide the starting points in the generation process and the models that are employed to provide the gradients required by the FGSM. In this thesis, almost all AEs are based on one of two datasets: \emph{GTSRB}\footnote{German Traffic Sign Recognition Benchmark} \cite{gtsrb-description} and \emph{ImageNet} \cite{imagenet-large-scale-hierarchical-image-database}. The former is a collection of traffic signs taken from German roads at various points during approach and the latter is a general-purpose aggregate of online images with very diverse classes. Both of these datasets are combined with a respective network. For GTSRB, a network based on \cite{multi-column-neural-network-gtsrb} was used to generate adversarial examples, while with ImageNet, the \emph{GoogLeNet} network from \cite{going-deeper-with-convolutions} was used. Both networks were trained on their respective data sets\footnote{But GoogLeNet due to both its size and the size of the data set was not trained from scratch, but the weights provided by the \emph{caffe} software were used instead}.

Either of these data sets have properties that make them interesting for AE research. GTSRB offers images with a high inherent difficulty, the object is often occluded, blurry or contains a stark light-dark contrast due to shadows. They are also frequently overexposed, have an almost prohibitively weak contrast or are simply very small. All these properties mean that if a network is able to perform well on the test set, it is likely to have exceptional invariance properties, adressing the conflict between generalization and the existence of adversarial examples. ImageNet, on the other hand, has very clean, full-size images, but features a large number of classes, so the effect of initial class and target (adversarial) class choice can be extensively studied. There is also a lot of \emph{object} variation in ImageNet. While images of different physical traffic signs may vary due to the aforementioned effects, the object that is to be classified always looks almost identical, by design. ImageNet, by contrast, groups lots of visually distinct entities into the same classes. The effect of this is a different type of generalization than GTSRB provides.
% TODO
% Elaborate on this: GTSRB is good for AEs since the occluded, shadow-containing, blurry overexposed images of different aspect ratio make for a net that strongly generalizes. ImageNet, on the other hand, has very clean, full-size images, but has a lot of classes, and there is a lot of "proper" variation in the images for each class (not just shadows and blurr etc.)

Although the specifics of AE generation vary between the two dataset-network pairs, because the different storage layouts on disk necessitate different bookkeeping code, the overall procedure is the same and will now be described before characteristics of each data set are discussed in sections \ref{sec:generation-large-sets:gtsrb} and \ref{sec:generation-large-sets:imagenet}. An algorithmic formulation is presented in algorithm \ref{alg:generate-ae-batches}.

% TODO state that this means that a class rep will be turned into the correct class (unless original and target are disjoint)
\begin{algorithm}
	\begin{algorithmic}
			\ForAll{oc $\in$ original\_class\_list}
				\State cr $\gets$ \Call{get\_random\_image}{class}
				\ForAll{tc $\in$ target\_class\_list}
					\State ae $\gets$ \Call{IterativeFGSM}{cr, tc, c, iterations} \Comment{save ae for later use}
				\EndFor
			\EndFor
	\end{algorithmic}
	\caption[Generating AE batches]{Generate AE batches using iterative FGSM}
	\label{alg:generate-ae-batches}
\end{algorithm}

The \emph{original\_class\_list} contains the classes from each of which a random class representative (\emph{cr}) should be picked. These class representatives will be transformed via iterative FGSM to each of the classes in the \emph{target\_class\_list}. For the GTSRB data set, the original and target class lists are identical and contain all $43$ classes, but for ImageNet this is infeasible because firstly it contains plethora of 1000 classes and secondly it uses the much larger GoogLeNet, so generating each individual adversarial example is much more computationally expensive. Hence, AE batches on ImageNet only use a subset of the available classes for \emph{original\_class\_list} and \emph{target\_class\_list}.

One complication needs to be mentioned at this point. The actual iterative FGSM code used in algorithm \ref{alg:generate-ae-batches} deviates slightly from the description in algorithm \ref{alg:make-adversarial}. Specifically, it terminated after reaching $90\%$ confidence on the target class. This was intended to save computation time, but as was discovered later and is described below, the turnout for such good AEs is very low, and such high confidences are only achieved near the end of iterative FGSM (if at all). This also means that the condition for this termination was hardly ever fulfilled, and thus it should have negligible impact, but it is pointed out here regardless for reproducibility reasons.

% TODO check Man vs Computer Benchmarking ML algorithms for traffic sign recognition.pdf to see if there is anything noteworthy to add about GTSRB
\subsection{GTSRB and GTSRB-Net}
\label{sec:generation-large-sets:gtsrb}
The GTSRB (German Traffic Sign Recognition Benchmark) database is a large accumulation of images of German traffic signs. The photos were taken from 10 hours of video footage while driving on the streets of germany in 2010. From the footage, a total of 51.839 images were extracted, split into a training set of 39.209 and a test set of 12.630 images, each of which contains exactly 1 sign. A few select images with good visibility are given in figure \ref{fig:gtsrb-overview}, more problematic instances are discussed below.

\begin{figure}[h!tb]
	\centering
	\includegraphics[width=\textwidth]{images/gtsrb/overview.png}
	\caption{Example images from GTSRB}
	\label{fig:gtsrb-overview}
\end{figure}

There are 43 classes with vastly different class frequencies, where the largest class contains more than 10 times as many images as the smallest classes. Image sizes vary greatly, ranging from $15\times15$ to $250\times 250$, but are generally small and not necessarily square. They come with at least a 10\% border around the traffic sign, and annotations are provided indicating a rectangular bounding box around the sign. The images are ordered into \emph{tracks}, where each track contains exactly 30 images of one physical traffic sign instance taken from various distances and angles, as the car approached it. There are at least 9 tracks per class. GTSRB poses a few challenges to classification models, and figure \ref{fig:gtsrb-challenges} is meant to illustrate some of them.

% TODO shadows and occlusion
\begin{figure}[h!tb]
    \centering
    \begin{subfigure}[b]{0.30\textwidth}
    		\centering
        \includegraphics[width=\textwidth]{gtsrb/problems/class23_00003_00029.jpg}
        \caption{Blur}
        \label{fig:gtsrb-challenges-blur}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.30\textwidth}
    		\centering
        \includegraphics[width=\textwidth]{gtsrb/problems/class26_00014_00029.jpg}
        \caption{Overexposure}
        \label{fig:gtsrb-challenges-overexposure}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.30\textwidth}
    		\centering
        \includegraphics[width=\textwidth]{gtsrb/problems/class19_00004_00029.jpg}
        \caption{Low contrast}
        \label{fig:gtsrb-challenges-contrast}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.30\textwidth}
    		\centering
        \includegraphics{gtsrb/problems/class09_00004_00029.jpg}
        \caption{Similarity}
        \label{fig:gtsrb-challenges-similar-a}
    \end{subfigure}
    \begin{subfigure}[b]{0.30\textwidth}
    		\centering
        \includegraphics{gtsrb/problems/class10_00025_00029.jpg}
        \caption{Similarity}
        \label{fig:gtsrb-challenges-similar-b}
    \end{subfigure}
    \begin{subfigure}[b]{0.30\textwidth}
    		\centering
        \includegraphics{gtsrb/problems/class28_00012_00003.jpg}
        \caption{Low resolution}
        \label{fig:gtsrb-challenges-resolution}
    \end{subfigure}
    \caption{Hand-picked images illustrating various impediments in the GTSRB dataset}
    \label{fig:gtsrb-challenges}
\end{figure}

As one can see, the dataset contains very blurry pictures (\ref{fig:gtsrb-challenges-blur}), as well as overexposed (\ref{fig:gtsrb-challenges-overexposure}) pictures and those severely lacking in contrast\footnote{If this image appears completely black, this is not by mistake. One can hardly make out the sign's borders when zooming in to the image file.} (\ref{fig:gtsrb-challenges-contrast}), all of which impede correct classification. Another kind of challenge arises from images like \ref{fig:gtsrb-challenges-similar-a} and \ref{fig:gtsrb-challenges-similar-b}. Even though they belong to different classes, they are very similar. This alone makes classification hard, but this is especially problematic here because thanks to occlusion and shadows, the data teaches the model that images which look completely different can and often do belong to the same class. Additionally, GTSRB contains some extremely tiny images (\ref{fig:gtsrb-challenges-resolution}), so that sometimes even humans can't be sure of the correct class.

% TODO table to summarize GTSRB properties

% TODO clarify that only one network is used, the cited paper uses a committee
In order to generate adversarial examples, a model that can provide the gradients for the (iterative) fast gradient sign method is required. Hence, a CNN was trained on the training data portion of GTSRB. It is based on \cite{multi-column-neural-network-gtsrb} and will be refered to as \emph{GTSRB-Net} within this thesis. The layout, hyper-parameter choices and training method for this network are detailed below.

GTSRB-Net operates on images containing RGB color information (3 channels) with a resolution of $48 \times 48$ pixels. Since the network accepts only one resolution, the GTSRB data needs to be adapted. Details regarding this will follow below. It is a convolutional neural network, where the convolutional part is succeeded by conventional, fully connected layers. Table \ref{tab:gtsrb-layout} specifies the layout details on a per-layer basis.

% TODO explain bias term
\begin{table}[h!tb]
	\centering
	\begin{tabular}{|l|lll|}
		\hline
		Layer & Type & Parameters & Activation function \\
		\hline
		0 & Input & $3 \times 48 \times 48$ & - \\
		1 & Convolution & 300 kernels of size $7 \times 7$, & tanh \\
		& & producing 100 maps & \\
		2 & Max-Pooling & Area $2\times 2$, stride 2 & - \\
		3 & Convolution & 15000 kernels of size $4 \times 4$, & tanh \\
		& & producing 150 maps & \\
		4 & Max-Pooling & Area $2\times 2$, stride 2 & - \\
		5 & Convolution & 37000 kernels of size $4 \times 4$, & tanh \\
		& & producing 250 maps & \\
		6 & Max-Pooling & $2\times 2$, stride 2 & - \\
		7 & Fully connected & 300 neurons, with bias term & tanh \\
		8 & Fully connected & 43 neurons, with bias term & softmax \\
		\hline
	\end{tabular}
	\caption{Layout of GTSRB-Net}
	\label{tab:gtsrb-layout}
\end{table}

The first operation applied by the network is a series convolutions with 300 kernels, each of which has a size of $7 \times 7$. Since a combination of 3 input maps and 3 kernels results in 1 output map (cf. figure \ref{fig:convolutional-layer-maps}), the total number of output maps is 100. This first convolutional layer is followed by a max-pooling layer, which reduces 4 values in a $2 \times 2$ area to 1 value, considerably reducing the map size. This pattern of a convolutional layer and a subsequent max-pooling layer is repeated 2 times. The respective pooling layers use identical parameters, but the kernel size in layers 3 and 5 is reduced to $ 4 \times 4$ and the number of kernels changes to produce 150 and 250 output maps, respectively. After the final max-pooling operation, a fully connected layer links the remaining values in the 250 maps to 300 neurons. Finally, this layer is followed up by another fully connected layer, this time with 43 neurons, one for each traffic sign class.

% TODO Glorot initialization
All activation functions in this network are hyperbolic tangents, except for the last layer, which uses a softmax activation in order to produce outputs that can be interpreted as class probabilities. As far as initialization is concerned, a uniform distribution was used for all parameters in the model, where values were drawn from the range $\brackets{-0.05, 0.05}$.

% TODO mention that rescaling was done with 'nearest' interpolation, and that this was an oversight
As indicated above, the circumstance that the GTSRB images vary in resolution complicates training and requires special handling. A neural network only accepts a fixed input size, so the traffic signs need to be scaled to one common resolution. A resolution that is too low discards valuable information from the larger images, while an overly large resolution wastes computation time and makes it unduly easy to find adversarial examples (cf. section \ref{subsec:ae-linear-explanation}). In this instance, a resolution of $48 \times 48$ was chosen\footnote{The data set provides annotations indicating where within the border contained in each of the images the traffic sign is located. Only this part was extracted and scaled to the aforementioned resolution}.

Apart from rescaling the images, one more preprocessing step was applied. As figure \ref{fig:gtsrb-challenges} demonstrates, especially when comparing \ref{fig:gtsrb-challenges-contrast} with the other images shown, there is a grave variability in the contrast of the images. In such a case, the performance can be greatly improved by normalizing the contrast, thereby relieving the CNN of an ordinary task so the learning can focus on the classification problem. Thus, contrast normalization\footnote{In the form of the \emph{autocontrast} function from the \emph{PIL} python libary} was applied to all images, both from the training and the test set. An example contrasting figure \ref{fig:gtsrb-challenges-contrast} and its preprocessed version are shown in figure \ref{fig:contrast-normalization}.

% TODO more images
\begin{figure}[h!tb]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
    		\centering
        \includegraphics{gtsrb/problems/class19_00004_00029.jpg}
        \caption{GTSRB image}
        \label{fig:contrast-normalization-before}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
    		\centering
        \includegraphics{gtsrb/problems/class19_00004_00029_normalized_contrast.jpg}
        \caption{Contrast normalized variant}
        \label{fig:contrast-normalization-after}
    \end{subfigure}
    \caption{Demonstration of contrast normalization}
    \label{fig:contrast-normalization}
\end{figure}

% TODO low_gamma and Xavier
Training of GTSRB-Net was done with a stochastic gradient descent, using a batch size of 16. The optimization ran for 14 epochs and tried to minimize the multinomial logistic loss. The weights were updated with a learning rate $\alpha$ according to equation \eqref{eq:caffe-learn-rate-exp}, where $b$ is the base learn rate of $0.1$, $\gamma = 0.999999$ and $i$ is the iteration, of which there were 548.926\footnote{Note that \emph{caffe}, the software used here, counts each presented image as its own iteration, even if they are combined into batches of size 16. Thus, 14 epochs for 39.209 images yields $14 \cdot 39.209 = 548.926$ iterations.}. In contrast to \cite{multi-column-neural-network-gtsrb}, no data augmentation was applied during training.

\begin{align}
	\alpha &= b \cdot \gamma^i \label{eq:caffe-learn-rate-exp}
\end{align}

% TODO plot of performance w.r.t epochs
% TODO FIXME realy 500 iterations?
% TODO mention how many AEs are in each set (43^2 = 1849)
% TODO state that the AEs are based off of the preprocessed (contrast normalized) images
With these settings and the data prepared as stated above, training resulted in an optimized model that achieves a top-1 accuracy of $97.75\%$ and a top-3 accuracy of $99.375\%$ on the test set after the last epoch. Based on this model and the preprocessed GTSRB training data, several sets of adversarial examples were created in accordance with algorithm \ref{alg:generate-ae-batches}. All $43$ classes were in the \emph{original} and \emph{target} class lists, and an iteration count of 500 was used for each of those sets, so the only variable parameter was the adaption strength coefficient $c$. All used values for $c$ and corresponding AE batch names are provided in table \ref{tab:gtsrb-ae-batches}.

% TODO are all of these batches used somewhere in this paper?
\begin{table}[h!tb]
	\centering
	\begin{tabular}{|l|l|l|l|l|l|}
		\hline
		AE batch name & c & iterations & $\abs{\text{\emph{original\_classes}}}$ & $\abs{\text{\emph{target\_classes}}}$ & number of AEs \\
		\hline
		gtsrb-ae-0.005 & 0.005 & 500 & 43 & 43 & 1849 \\
		gtsrb-ae-0.025 & 0.025 & 500 & 43 & 43 & 1849 \\
		gtsrb-ae-0.037 & 0.037 & 500 & 43 & 43 & 1849 \\
		gtsrb-ae-0.05 & 0.05 & 500 & 43 & 43 & 1849 \\
		gtsrb-ae-0.1 & 0.1 & 500 & 43 & 43 & 1849 \\
		gtsrb-ae-0.5 & 0.5 & 500 & 43 & 43 & 1849 \\
		gtsrb-ae-0.9 & 0.9 & 500 & 43 & 43 & 1849 \\
		\hline
	\end{tabular}
	\caption[Description of AE batches generated on GTSRB / GTSRB-Net]{Generated AE batches on GTSRB / GTSRB-Net}
	\label{tab:gtsrb-ae-batches}
\end{table}

\subsection{ImageNet and GoogLeNet}
\label{sec:generation-large-sets:imagenet}
% TODO citation for WordNet
ImageNet \cite{imagenet-large-scale-hierarchical-image-database} is an enormous image database with a broad application spectrum. ImageNet is quite organic in nature, but as of the time of this writing contains over 14 million images. Its structure is based on \emph{WordNet}, which structures words according to their similarity and hierarchy. The hierarchical structure is disregarded, though, all images are considered to belong to exactly one class. In fact, the entirety of ImageNet is too humongous to be considered here, so this thesis focuses on the \emph{ILSVRC2012} subset, used in the 2012 iteration of the \emph{ImageNet Large Scale Visual Recognition Challenge}, from which the acronym is derived. This subset already provides a non-hierarchical structure, with an abundance of 1000 distinct classes. It contains 1.281.167 images, a split into training and test data is not provided. Each class contains between 732 and 1300 images, with the cast majority tending to the upper limit.

ImageNet comprises RGB color images of varying size and very diverse aspect ratio. This requires the data to be scaled, and possibly stretched and squashed, during training and deployment. The images also contain much more generous borders than those of GTSRB, or rather show the item to be classified in a broader environment, making the task of classification even harder\footnote{In general, ImageNet contains annotations, some with bounding boxes, but they are not included in the ILSVRC2012 set.}. Not surprising given a data set with 1000 classes, the classes themselves are quite diverse, as is illustrated in figure 

% TODO scale images down even more so they take up less space
% TODO FIXME add class names
\begin{figure}[h!tb]
    \centering
    \begin{subfigure}[b]{0.30\textwidth}
    		\centering
        \includegraphics[width=\textwidth]{imagenet/examples/n01820546_31.JPEG}
        \caption{n01820546}
        \label{fig:imagenet-examples-1}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.30\textwidth}
    		\centering
        \includegraphics[width=\textwidth]{imagenet/examples/n03530642_10006.JPEG}
        \caption{n03530642}
        \label{fig:imagenet-examples-2}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.30\textwidth}
    		\centering
        \includegraphics[width=\textwidth]{imagenet/examples/n02281406_10006.JPEG}
        \caption{n02281406}
        \label{fig:imagenet-examples-3}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.30\textwidth}
    		\centering
        \includegraphics[width=\textwidth]{imagenet/examples/n04597913_10021.JPEG}
        \caption{n04597913}
        \label{fig:imagenet-examples-4}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.30\textwidth}
    		\centering
        \includegraphics[width=\textwidth]{imagenet/examples/n02782093_1001.JPEG}
        \caption{n02782093}
        \label{fig:imagenet-examples-5}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.30\textwidth}
    		\centering
        \includegraphics[width=\textwidth]{imagenet/examples/n03937543_10012.JPEG}
        \caption{n03937543}
        \label{fig:imagenet-examples-6}
    \end{subfigure}
    \caption[ImageNet class diversity]{Images from ILSVRC2012 subset of ImageNet, illustrating class diversity.}
    \label{fig:imagenet-examples}
\end{figure}

% TODO Is 27 layers deep (22 if pooling not counted)
The network trained on this data set and used to generate adversarial examples from it is GoogLeNet, introduced in \cite{going-deeper-with-convolutions} as part of the \emph{Inception} architecture for CNNs. Even though the paper makes reasonable claims that the architecture uses available parameters very efficiently, it is still a bulky network with 27 layers\footnote{The paper mentions 22, not counting pooling layers since they are parameterless}. All layers use ReLU activation functions, except for the output, which uses a softmax function. The weight initialization distribution is not specified\footnote{Although they did mention that an ensemble they constructed used the same initialization for all models} in \cite{going-deeper-with-convolutions}.

Training a network of this size on a sufficiently large portion of ImageNet was infeasible with the available hardware infrastructure and time budget. Fortunately, a suitable, already trained model is available from \cite{caffe-model-zoo}. This network accepts 3-channel RGB images with a resolution of $224 \times 224$ as input data. Weights were initialized according to \cite{glorot-understanding-deep-nn-training}. The model was trained without data augmentation for 60 epochs, achieving a top-1 accuracy of $68.7\%$.

% TODO FIXME what about the ..._progress_... sets?
% TODO About the 3.0 set: With only 100 AEs it is considerably smaller than the other two, but since each of those contributes to the goal of this set it is of comparable importance.
Using this model, 3 batches of adversarial examples were generated from the ImageNet data set as per algorithm \ref{alg:generate-ae-batches}. The first one used a random subset of 50 classes for both the \emph{original\_class\_list} and \emph{target\_class\_list} so as to completely cross-generate AEs, at least on a small subset. Another batch used one randomly chosen class for the \emph{original\_class\_list} and used the entirety of ImageNet's 1000 classes as the \emph{target\_class\_list}, so an adversarial example was generated at least once for each class. It is important to understand that this implies only one AE per target class, all generated from the same class representative. This considerably weakens significance of any conclusions drawn, but was feasible with the resources at hand. Both of the aforementioned batches use a value of $c = 0.9$, which has proven to be reasonable on ImageNet in section \ref{sec:fundamental-properties}, and $500$ iterations. A third batch was created for the purpose of amplifying the properties of adversarial examples examined in the following sections, even at the cost of visual integrity. This used a value of $c = 3.0$, also with $500$ iterations on a 10 class subset for both original and target class lists. These batches are summarized and given a name in table \ref{tab:imagenet-ae-batches}.

\begin{table}[h!tb]
	\centering
	\begin{tabular}{|l|l|l|l|l|l|}
	\hline
		AE batch name & c & iterations & $\abs{\text{\emph{original\_classes}}}$ & $\abs{\text{\emph{target\_classes}}}$ & number of AEs \\
		\hline
		imagenet-ae-50 & 0.9 & 500 & 50 & 50 & 2500 \\
		imagenet-ae-1000 & 0.9 & 500 & 1 & 1000 & 1000 \\
		imagenet-ae-10-3.0 & 3.0 & 500 & 10 & 10 & 100 \\
	\hline
	\end{tabular}
	\caption[Description of AE batches generated on ImageNet / GoogLeNet]{Generated AE batches on ImageNet / GoogLeNet}
	\label{tab:imagenet-ae-batches}
\end{table}


%%%%% Dataset:
%% From when?
%% Number of images (and train/test split)
%% Number of classes
%% How many images per class?
%% Contains color images
%% Varying sizes and aspect ratio
%% Show some images
%% Unbalanced class frequencies?
%% Border around content?, Bounding boxes?
%%%%% Network:
%% Network parameters (layout, initialization, activation functions, number and size of filters)
%% Preprocessing, resizing, contrast normalization
%% Training parameters (loss, learn rate, decay, batch size, epoch count)

% TODO
% Present a few AEs for both GTSRB and ImageNet
% Talk about how many of those actually turned out to be AEs. In general, show some nice statistics for them, like a cumulative distribution over the confidence etc.









\section{Frequency spectra of adversarial examples}
\label{sec:spectra}
% TODO talk about Modelling the Power Spectra of Natural  Images: Statistics and Information.pdf
% TODO talk about Amplitude Spectra of Natural Images.pdf
There are several ways to make use of the AE batches generated in section \ref{sec:generation-large-sets}. One of them is to analyze the frequency spectra of the original images and the adversarial examples and compare them. Spectra often prove useful when processing or studying signals since they have mathematical properties that greatly simplify some operations\footnote{For instance when accelerating convolutions}, and offer a perspective on the data that is vastly different from the one in position space.

% TODO HSV or LAB space instead of grayvalue
First of all follows a description of the approach taken to compute the spectra, how the data was normalized and then further processed. Computations were performed on grayvalue variants of the images, the 3 color channels were additively combined to one channel\footnote{No weighting was performed, i.e. giving the red channel a preference over the others}. To avoid issues arising from different scaling of the data, all images were normalized to norm $1$ by scaling each element with the factor $d$ according to equation \eqref{eq:image-normalization}. This is equivalent to scaling the flattened vector containing the pixel values to a length of one, measured by the euclidian norm. As a final step, a 1 row or column wide strip was cropped where necessary to make image dimensions odd, allowing for point-wise symmetry in the spectra.

\begin{align}
	d &= \frac{1}{\displaystyle \sqrt{\sum_{i \in I}{i^2}}} \label{eq:image-normalization}
\end{align}

Based on this normalized data the spectra were computed via an unscaled\footnote{see \cite{numpy-fft} for details} two-dimensional FFT for each image, followed by taking the absolute value of the result. To get a combined spectrum for all images, a simple averaging operation was performed. This process was executed separately for the original data and the adversarial examples.

\subsection{ImageNet and GoogLeNet}
% TODO FIXME do not use a 20.000 image subset, but the whole thing
The first spectra to be inspected come from the imagenet-ae-50 data set. For the images shown in figure \ref{fig:imagenet-ae-50-spectra}, only those adversarial examples contributed to the AE spectrum which obtained a confidence of $50\%$ or higher. This way, it is certain that the network decision corresponds to the desired class\footnote{One could also have identified those AEs with confidences below $50\%$ which successfully trick the network at creation time without additional computational cost, but this overlooked at the time.}. There were 287 suitable AEs in the pool. The spectrum of the original ImageNet data was approximated by the spectrum of a 20.000 image subset.

% TODO reduce space between images so they can be bigger
% TODO break up into 3 subfigures
\begin{figure}[h!tb]
	\centering
	\includegraphics[width=\textwidth]{images/spectra/imagenet-ae-50-minconfidence-0dot5-maxorig-20000-spectra.png}
	\caption[Log spectra of ImageNet and adversarial examples]{Element-wise logarithm of the spectra of ImageNet and imagenet-ae-50, and their difference (adversarial - original)}
	\label{fig:imagenet-ae-50-spectra}
\end{figure}

The original spectrum conforms with the observation that, at least in expectation, natural image spectra follow a $\frac{1}{f}$ pattern, where $f$ is the frequency\cite{relations-between-natural-image-statistics-and-cortical-cells}. The adversarial spectrum is considerably more noisy, owing to the smaller amount of data, but also seems to follow this general pattern. However, it seems like an area around the center axes that gets wider as the frequency increases exposes unusually high amplitudes. The visualization of the two image's difference makes this even more clear. Rather than the uncorrelated noise that would be expected, one can somewhat clearly discern wedges radiating out from the origin. These irregularities motivate the analysis below.

% TODO maybe add pseudocode for this (or maybe describe the process in mode detail
Going one step further than visual inspection, a known method for inspecting image spectra properties is to focus on the activity distribution w.r.t the frequency. To this end, the possible distances from the origin to the corners of the spectrum array were divided into $158$ equidistant bins, each containing a scalar number. Then, for each value in the spectrum, its distance was computed and the value was added to the appropriate bin. This corresponds to adding up the values of points on concentric circles around the origin, but each value is added exactly once. Obviously, some circles get more points allocated to them than others, so afterwards each bin was divided by the number of values that were assigned to it. This done for all 3 arrays shown in figure \ref{fig:imagenet-ae-50-spectra} and the result is plotted on a log-log scale in figure \ref{fig:imagenet-ae-50-frequency}.

% TODO label axes
\begin{figure}[h!tb]
	\centering
	\includegraphics[width=\textwidth]{images/spectra/imagenet-ae-50-minconfidence-0dot5-maxorig-20000-distance-158-bins.png}
	\caption[Log spectra of ImageNet and adversarial examples, frequency analysis]{Activity distribution in ImageNet and imagenet-ae-50, w.r.t frequency}
	\label{fig:imagenet-ae-50-frequency}
\end{figure}

% TODO checking this constructive interference theory is good stuff for the conclusion
The distributions for the individual spectra again mostly conform to the $\frac{1}{f}$ relationship, and no clear disparity is visible. The plot of the difference confirms that the differences are weak in amplitude, but it is interesting to notice that they don't appear to be indiscriminate. Rather, it seems that adversarial examples have systematically higher activation in the low frequency region and lower activation in the high-frequency region, except for a small spike near the end. This is a surprising result given that the visual inspection of extremely modified images appears to be high-frequent in nature (cf. figure \ref{fig:ae-panda-dresser-1dot8}). A possible explanation for this is that the low-frequency components are accumulations of many different directions and are thus not visibly noticeable, while the few high-frequency components use constructive interference to produce the strong observed noise patterns. A possible way to test this is to analyze the positions of the spots for correlations. If they are the result of constructive interference, they should be much more strongly correlated than random noise.

The observation of outwards radiating wedges in figure \ref{fig:imagenet-ae-50-spectra} also suggests a second method of analysis. Rather than focusing on the distance to the origin, one might look for differences in the radial pattern. Thus, a procedure analogous to the one described above was performed, but instead of grouping the array elements by frequency, they were grouped by angle. 16 bins were used and similarly normalized by the number of values going into them. The result can be inspected in figure \ref{fig:imagenet-ae-50-angle}.

% TODO label axes
% TODO Use angles in radians as axis ticks
\begin{figure}[h!tb]
	\centering
	\includegraphics[width=\textwidth]{images/spectra/imagenet-ae-50-minconfidence-0dot5-maxorig-20000-angle-16-bins.png}
	\caption[Log spectra of ImageNet and adversarial examples, angular analysis]{Activity distribution in ImageNet and imagenet-ae-50, w.r.t angle}
	\label{fig:imagenet-ae-50-angle}
\end{figure}

The original and adversarial plots are similar, though one might already see some of the more pronounced differences. The difference plot shows a fair amount of discontinuity along the angles. Bin angles were not specifically designed to not split the visible wedges between bins, but since they seem to be aligned with the coordinate axes this may have been avoided by chance (which is desirable since the goal is to inspect their significance).

% TODO test if this is true and then incorporate it:
% The spectra and the corresponding distance and radial distributions for imagenet-ae-1000 are similar and therefore not shown.
% alternatively:
% They are very noisy because only one start image.

Next, the same methods will be applied to the imagenet-ae-10-3.0 batch. Even though the perturbations there are clearly visible, the data set was created in hopes that some properties of adversarial images are particularly strong there, including those related to changes in the image spectra. Since the confidence is generally very high on this AE batch, only AEs with a confidence of $90\%$ or higher were included, comprising 56 images. The spectra and their difference are shown in figure \ref{fig:imagenet-ae-10-3dot0-spectra}.

% TODO reduce space between images so they can be bigger
% TODO break up into 3 subfigures
\begin{figure}[h!tb]
	\centering
	\includegraphics[width=\textwidth]{images/spectra/imagenet-ae-10-3dot0-minconfidence-0dot9-maxorig-20000-spectra.png}
	\caption[Log spectra of ImageNet and strong adversarial examples]{Element-wise logarithm of the spectra of Imagenet and imagenet-ae-10-3.0, and their difference (adversarial-original)}
	\label{fig:imagenet-ae-10-3dot0-spectra}
\end{figure}

Somewhat surprisingly, while the trend for pronounced wedge-shaped regions emanating from the origin is still visible in the difference, its strength has considerably weakened, rather than strengthened. There are two possible causes for this: Either the pattern only emerges for true adversarial examples that visually closely resemble their original, or the noise in the adversarial spectrum, which is considerably higher than in figure \ref{fig:imagenet-ae-50-spectra}, owing to the smaller number of AEs, is drowning out systematic differences. The frequency and angle distributions might shed some light on this, the former of which is plotted in figure \ref{fig:imagenet-ae-10-3dot0-frequency}.

% TODO label axes
\begin{figure}[h!tb]
	\centering
	\includegraphics[width=\textwidth]{images/spectra/imagenet-ae-10-3dot0-minconfidence-0dot9-maxorig-20000-distance-158-bins.png}
	\caption[Log spectra of ImageNet and strong adversarial examples, frequency analysis]{Activity distribution in ImageNet and imagenet-ae-10-3.0, w.r.t frequency}
	\label{fig:imagenet-ae-10-3dot0-frequency}
\end{figure}

While the downwards trend observed in figure \ref{fig:imagenet-ae-50-frequency} is less pronounced here and subject to a more dominant to-and-fro movement, the overall shape is comparable, especially in the high frequency part. What is mentionable about this curve is the circumstance that all values lie in the negative range. This implies that the log spectrum of the adversarial examples was smaller than that of the original images at every distance\footnote{This does not mean that every value in the spectrum was smaller, just the sum of all values at a specific distance}. The corresponding angle distribution for imagenet-ae-10-3.0 is shown in figure \ref{fig:imagenet-ae-10-3dot0-angle}.

% TODO label axes
% TODO Use angles in radians as axis ticks
\begin{figure}[h!tb]
	\centering
	\includegraphics[width=\textwidth]{images/spectra/imagenet-ae-10-3dot0-minconfidence-0dot9-maxorig-20000-angle-16-bins.png}
	\caption[Log spectra of ImageNet and strong adversarial examples, angular analysis]{Activity distribution in ImageNet and imagenet-ae-10-3.0, w.r.t angle}
	\label{fig:imagenet-ae-10-3dot0-angle}
\end{figure}

While the distributions on their own are very similar to those in \ref{fig:imagenet-ae-50-angle}, their difference is rather dissimilar. First of all, the values again all reside in the negative range, so the amplitudes in the adversarial spectra seem to be smaller across all directions and distances for imagenet-ae-10-3.0. However, even when accounting for the difference in mean value of the two plots, the overall shape is different. As stated above, this might be an effect of the low sample size for adversarials, or the strongly altered images of this AE batch expose different properties\footnote{It may also be a combination of both factors.}.

\subsection{GTSRB and GTSRB-Net}
% TODO FIXME be clear that 'original' spectra here were computed from training data
Apart from the adversarial examples spawned from ImageNet, the spectra of the GTSRB AEs were also computed. This should lead to a more varied inspection of adversarial image properties. In the following, the same analysis as above will be performed for the gtsrb-ae-0.025 batch. The adversarial spectrum was computed only from AEs with a confidence of at least $50\%$, which amount to 136 images. The original spectrum represents the entire training data of 39.209 images. The spectra can be seen in figure \ref{fig:gtsrb-ae-0dot025-spectra}

% TODO reduce space between images so they can be bigger
% TODO break up into 3 subfigures
\begin{figure}[h!tb]
	\centering
	\includegraphics[width=\textwidth]{images/spectra/gtsrb-ae-0dot025-minconfidence-0dot5-maxorig-0-spectra.png}
	\caption[Log spectra of GTSRB and adversarial examples]{Element-wise logarithm of the spectra of the GTSRB training set and gtsrb-ae-0.025, and their difference (adversarial-original)}
	\label{fig:gtsrb-ae-0dot025-spectra}
\end{figure}

Since the resolution of GTSRB-Net is only $48 \times 48$ pixels, it is quite hard to discern notable features beyond the fact that, as expected, the spectra adhere to a $\frac{1}{f}$ pattern. If viewed from a distance, the inner parts of the quadrants of the difference appear to be brighter than the axes through the origin, but a more thorough examination is provided by the amplitude distribution over the distance to the origin plotted in figure \ref{fig:gtsrb-ae-0dot025-frequency}.

% TODO label axes
\begin{figure}[h!tb]
	\centering
	\includegraphics[width=\textwidth]{images/spectra/gtsrb-ae-0dot025-minconfidence-0dot5-maxorig-0-distance-33-bins.png}
	\caption[Log spectra of GTSRB and adversarial examples, frequency analysis]{Activity distribution in the GTSRB training set and gtsrb-ae-0.025, w.r.t frequency}
	\label{fig:gtsrb-ae-0dot025-frequency}
\end{figure}

Here, the frequencies have been apportioned into 33 bins. It is notable that the plots for both the original and adversarial spectrum less accurately resemble a straight line. Small sample size is an unlikely explanation, because straighter lines were produced by the even smaller mount of 56 AEs leading to figure \ref{fig:imagenet-ae-10-3dot0-frequency}, and because the original distribution is based on the entire GTSRB training set. The remaining explanations are that the bumps are indeed characteristic of traffic signs, or that the contrast normalization procedure performed on the GTSRB images caused them. The frequency distribution difference is of special interest here because it shows an opposite trend to figures \ref{fig:imagenet-ae-50-frequency} and \ref{fig:imagenet-ae-10-3dot0-frequency}: The difference tens more to the lower end for the low frequencies and progresses higher as the frequencies increase. What all these plots have in common is a out-of-line upwards spike near the high-frequency end of the spectrum\footnote{Though it is weaker on the imagenet-ae-10-3.0 set}. Another major difference is the fact that this plot stays consistenty \emph{above} 0, so adversarial amplitudes are larger than their original counterparts at all frequencies.

The above indicates that the spectral properties of adversarial examples that set them apart from the original data distribution is specific to the data set, or even the domain which the data set encompasses. One might also suspect the model as a major determinant. This has not been investigated, but since adversarial examples generalize across models, it doesn't seem likely. Take note that, while \cite{intriguing-properties-of-neural-networks} has shown that AEs generalize across \emph{subsets} of the same training data set, generalization across different data sets belonging to the same domain has so far not been demonstrated.

% TODO Really "lastly"?
Lastly, the angular distribution will be analyzed. Figure \ref{fig:gtsrb-ae-0.025-angle} contains plots for the training set, gtsrb-ae-0.025 and their difference. As usual, the bins containing the coordinate axes can collect larger amplitudes than the rest. The difference is again strictly positive, but the trends on can observe there do not seem to correlate with those of the ImageNet angular distribution differences.

% TODO label axes
% TODO Use angles in radians as axis ticks
\begin{figure}[h!tb]
	\centering
	\includegraphics[width=\textwidth]{images/spectra/gtsrb-ae-0dot025-minconfidence-0dot5-maxorig-0-angle-16-bins.png}
	\caption[Log spectra of GTSRB and adversarial examples, angular analysis]{Activity distribution in the GTSRB training set and gtsrb-ae-0.025, w.r.t angle}
	\label{fig:gtsrb-ae-0.025-angle}
\end{figure}

% TODO
% What are spectra
%% How are they computed (in general, fft)
%% I actually use the amplitude spectrum
%% Describe image normalization
%% State the displayed images are the logarithms because visiblity.
%% Layout of frequency space
% How do they normally look? Somewhat bell-shaped (feat. that paper)
%% How are they computed here (averaging)
%% Description of how the AEs were created (parameters etc.)
%% How to calculate distribution w.r.t distance, how is it normalized?
%% Analysis w.r.t distance
%% How to calculate distribution w.r.t angle, how is it normalized?
%% Analysis w.r.t angle








\section{Class dependence of generation success}
\label{sec:class-dependence-generation-success}
% confusion matrix (see http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset for confusion matrix example}
Section \ref{sec:fundamental-properties} revealed that the confidence a generated AE can attain is heavily influenced by the choice of target class. Transforming the panda into an acorn for instance was easy even with as few as 10 iterations, leading to a confidence of $86.17\%$ ($94.49\%$ with 100 iterations). All attempts of turning the panda into a dresser however, proved to be futile, unless one is willing to accept drastic, salient changes to the image. Manual sampling also revealed that the choice of source class had a significant impact, it seems that some classes are more easily turned into specific things than others.

In an effort to make these observations more systematic and meaningful, the imagenet-ae-50 batch was inspected more closely. As described in section \ref{sec:generation-large-sets:imagenet}, it contains an AE for every pair of source and target class (where the source class is represented by a randomly chosen representative), for a ImageNet subset of 50 classes. This structure lends itself to a representation akin to a confusion matrix, as provided by figure \ref{fig:imagenet-ae-50-confusion-matrix}. It is crucial to keep in mind that its significance is strongly limited by having only one AE for each pair, and one source image per source class.

\begin{figure}[h!tb]
	\centering
	\includegraphics[width=\textwidth]{images/confusion-matrices/imagenet-ae-50.png}
	\caption[Confidence of GoogLeNet on ImageNet, by source and target class.]{Confidence of GoogLeNet on imagenet-ae-50, broken down by source and target class. White indicates high confidence.}
	\label{fig:imagenet-ae-50-confusion-matrix}
\end{figure}

Unsurprisingly, the diagonal is almost filled with high-confidence elements. It represents the attempts to turn an image into the correct class as designated by its true label. The ballpoint pen is a notable exception. The very low confidence value indicates that not only did the network fail to classify it correctly, it is also hard to change the image such that the network makes a correct prediction.

A particularly noticeable feature of the map are the horizontal lines. Most pronounced are those corresponding to the source classes \emph{assault rifle}, \emph{monastery}, \emph{conch} and \emph{file}, but other rows expose a similar trend. They indicate that the corresponding source classes can be much more effectively turned into adversarial examples of a wide range of target classes than other source classes. As a consequence, the distribution of adversarial examples in input space seems to be skewed. There exist points, for example those represented by the assault rifle image, that are surrounded by AEs from a large number of classes, while the dresser image\footnote{refered to as a chiffonier in ImageNet} has no adversarial examples nearby\footnote{The dresser used as a class representative in imagenet-ae-50 was a different one from the one inspected in section \ref{sec:fundamental-properties}.}. Another implication of this structure is that the spectra computed in section \ref{sec:spectra} are biased towards certain classes, since only actually adversarial examples, i.e. those which convince the network, were used. If more data were available, the analysis might benefit from taking a stratified sample set from the available AEs.

The reverse does not seem to apply. The confidence among the columns in figure \ref{fig:imagenet-ae-50-confusion-matrix} seems to be much more evenly distributed, albeit noisy. There is no class into which most other classes are readily transformed. There are, however 2 classes that appear to be particularly hard to mimic: the columns for the lorikeet and red-breasted merganser are almost entirely black. So, while some points in input space are surrounded by adversarial examples of most classes, which classes are missing does not seem to be random. 

As indicated above, a major drawback of performing this analysis on the imagenet-ae-50 batch is the fact that each confidence value is only generated from one data point. Fortunately, a larger number of AE batches were created for GTSRB that all have the same source and target classes\footnote{Namely all classes in GTSRB.}. Most importantly, since the class representatives are chosen at random, they vary across the different batches. While the batches also share an iteration count of 500, their value for $c$ differs quite broadly. However, the results presented in section \ref{sec:fundamental-properties} give reason to believe that a larger/smaller value of $c$ does not induce systematic bias in favor or some source-target class pairs, but rather increases/decreases confidences uniformly. In other words, it might make trends like those in figure \ref{fig:imagenet-ae-50-confusion-matrix} less visible by adding entire sets of high or low confidence, but should not create new trends. A confusion-matrix like plot for a combination of the GTSRB batches is presented in figure \ref{fig:gtsrb-ae-multiple-confusion-matrix}.

\begin{figure}[h!tb]
	\centering
	\includegraphics[width=0.8\textwidth]{images/confusion-matrices/gtsrb-ae-multiple-0005-0025-0037-005.png}
	\caption[Confidence of GTSRB-Net on GTSRB, by source and target class]{Confidence of GTSRB-Net on combined gtsrb-ae-\{0.005, 0.025, 0.037, 0.05\}, broken down by source and target class. White indicates high confidence.}
	\label{fig:gtsrb-ae-multiple-confusion-matrix}
\end{figure}

With 4 AEs per element, this figure is a bit more significant than figure \ref{fig:imagenet-ae-50-confusion-matrix}. The observations made previously are found reaffirmed here. One can clearly see that some classes are successfully transformed, while others, like the two lowermost classes, resist change toward any other class. One caveat to be aware of here is that due to a difference in the $c$ parameter, the exact grayvalues are not readily interpretable. However, the overall structure that is visible still retains its meaning.

% TODO show representatives of those classes that are easily / hardly transformed.











\section{Predicting the final confidence}
\label{sec:predicting-final-confidence}
In section \ref{subsec:iterative-properties}, particularly around figure \ref{fig:confidence-evolution}, the development of network confidence on the source class and adversarial class were probed. It was observed that adversarial confidence roughly follows a sigmoidal shape during a successful run. Furthermore, it is evident that the curve does not approach a value of $1.0$ for an unsuccessful run. Since generating lots of adversarial examples is computationally expensive, this gives rise to an opportunity: If the initial part of the confidence development is indicative of the final confidence, one can give up on hopeless cases and save the computation time for more promising ones.

In the following, a method used to probe the predictive power of the confidence progress during the initial iterations is presented. The idea is to fit a parameterized function to the first part of the curve. One of the free parameters indicates the final confidence and can be used to estimate the change of successful AE generation if the process is continued. The others give some freedom to the shape of the function, allowing for a meaningful fit. Since the successful conversion plots resemble a sigmoidal function, a parameterized sigmoid as per equation \eqref{eq:parameterized-sigmoid} is used.

\begin{align}
	\sigma \braces{x, \alpha, \beta, x_0} &= \frac{\beta}{1 + e^{-\alpha \braces{x - x_0}}} \label{eq:parameterized-sigmoid}
\end{align}

The parameter $x$ indicates the iteration, while $\alpha$, $\beta$ and $x_0$ are free parameters. Here, $x_0$ corresponds to the turning point of the curve. It normally rests at $0$, but since a rise in confidence may occur much later, this parameter allows the function to approximate the data in a meaningful way. A similar purpose is fulfilled by $\alpha$, which controls the gradient at the turning point. The remaining free parameter $\beta$ indicates the asymptotic height of the sigmoid and can thus serve as a predictor for the value at which the confidence will saturate.

There is obviously a problematic side to this approach. If the data indeed follows a sigmoidal path and the function is fitted based on the foremost data points, then the chance of success appears to be very low. The foremost part of the sigmoidal curve is the least significant bit because it closely resembles a straight line. Predicting the eventual height of the function from that seems doubtful. However, judgment based on visual impressions is easily led astray, so measurements are presented below.

For the training portion, the first $40\%$ of the curves have been chosen. While this means that $40\%$ of the execution time would be spent even if the pair of source image and target class is hopeless, given the challenge outlined above it seems appropriate to use a rather large value. The sigmoid was fitted to this portion of the data via the Trust Region Reflective least squares algorithm\footnote{specifically, \emph{SciPy's} implementation of it}, and starting values of $\alpha = 0.002$, $\beta = 0.5$ and $x_0 = 70$ were chosen after visual inspection of some of the confidence progress curves. The fitting was performed under the constraints that $\alpha$ must be positive and $\beta \in \brackets{0, 1}$. Select curves from AE generation on ImageNet and the corresponding fitted functions are shown in figure \ref{fig:progress-fit}.

\begin{figure}[h!tb]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
    		\centering
        \includegraphics[scale=0.5]{progress/imagenet_0dot9c_100iter_800samples-success.png}
        \caption{Fit on successful AE generation}
        \label{fig:progress-fit-success}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
    		\centering
        \includegraphics[scale=0.5]{progress/imagenet_0dot9c_100iter_800samples-failure.png}
        \caption{Fit on failed AE generation}
        \label{fig:progress-fit-failure}
    \end{subfigure}
    \caption[Parameterized sigmoidal fit to confidence evolution]{Parameterized sigmoidal fit to confidence evolution. Source images were taken from ImageNet and transformed to random classes for 100 iterations with $c = 0.9$. The first 40 points were used to fit the functions.}
    \label{fig:progress-fit}
\end{figure}

The curves in figure \ref{fig:progress-fit-success} exhibit a strong difference. While the blue curve is approximated very well, the green one's confidence after 100 iterations is severely underestimated. The most grave cause for this is likely the point at which the presumed sigmoid starts its ascent. The blue curve starts with much earlier with its slope, and also begins to settle down a good while before the process terminates iterations. For the green curve, the optimization has much less information to work with, since the curve is almost completely flat during the first 60 steps. In figure \ref{fig:progress-fit-failure} the same trend can be observed. The orange curve starts to rise earlier\footnote{even though it is later overtaken by the red one}, leading to a higher fit, but not a better one since it overshoots.

Besides the examples examined above, it is advisable to quantify the error in the fit in order to gauge its predicitve capability. Therefore, the deviation between observed curve and fit on the remaining, not fitted part was computed and averaged across the iterations and 800 curve-fit pairs. This yielded a mean deviation of $\approx 0.03978$. While a discrepancy of $3\%$ is unacceptable for some purposes, trying to decide whether to continue with a specific run of iterative FGSM actually allows for a rather larger margin of error. As long as the chance that an AE will be able to trick the network is high enough, it is not pivotal what the exact confidence will be. However, while the average across the remaining iterations describes the quality of the fit, only the last iteration is decisive for this purpose. Thus, when the average is only taken over all last iteration differences of the 800 curves, one obtains a deviation of $0.1093$. While this is only slightly more worrying, it still does not address the full scale of the problem. What is not obvious form figure \ref{fig:progress-fit} is the fact that most source image and target class pairs don't lead to a successful adversarial example, so the distribution is heavily skewed towards curves like \ref{fig:progress-fit-failure}, unduly lowering the mean deviation.

A more meaningful evaluation can be made by splitting the curves into two sets, those that result in an AE above a certain minimal confidence and those who don't. Then, one can use the fit's value for the last iteration as a prediction of whether or not the transformation process will reach that goal. A value of $0.5$ was chosen for the minimum confidence, leading to a precision of $0.5741$ and a recall of $0.3563$. The relevant quantities are given in table \ref{tab:prediction-quantities}. This means that about $43\%$ of images predicted to reach a confidence of $0.5$ will actually fall short of that goal, which is not very problematic since this is only a very small portion of the images anyway. The low recall is a far greater problem. Since successful AEs are rather rare, passing on about $65\%$ of them is a prohibitively high price to pay if one still has to perform $40\%$ of the iterations on those $86\%$ true negatives.

\begin{table}[h!tp]
	\centering
	\begin{tabular}{|ll|ll|}
		\hline
		& & \multicolumn{2}{c|}{Prediction} \\
		& & $\geq 0.5$ & $< 0.5$ \\
		\hline
		\multirow{2}{*}{Data} & $\geq 0.5$ & 31 & 56 \\
		& $< 0.5$ & 23 & 690 \\
		\hline
	\end{tabular}
	\caption[Image counts reaching a threshold as observed and predicted by curve fitting]{Numbers of images reaching a minimal threshold of $0.5$, broken down by observed truth and prediction.}
	\label{tab:prediction-quantities}
\end{table}

% TODO
% Is the deviation systematic in one direction?
% This should be done for other iteration counts.











\section{Conclusion}
Machine learning techniques offer a lot of opportunities to improve our lives and relieve humans of dull labor. They are becoming increasingly popular, are in our phones, recognize faces in photos in social networks and will soon drive our cars for us. Deep CNNs in particular have been exceedingly successful on image related tasks, sometimes even beating humans. But despite their robustness and performance on ever more challenging tasks, adversarial examples expose a considerable gap between the reality of modern machine learning techniques and the capabilities expected of them.

Even though CNNs supposedly abstract away from minute specifics in input images, thereby attaining great generalization and invariance properties, there exists images with imperceptible changes from correctly classified ones that elicit wrongful high-confidence predictions. These adversarial examples demonstrate unexpected and undesirable traits of these models. They can be attributed to their linear nature and don't require assumptions on the nonlinearity of the network. Instead, the size of the input data is key, where a high dimensionality significantly simplifies the process of generating adversarial examples.

Adversarial examples can be efficiently generated via iterative FGSM. Performing a gradient descent on the input space can notably improve network confidence on the adversarial class at the expense computation time, but is still feasible, especially since the first few iterations added are by far the most valuable. More iterations can improve the adversarial properties of the resulting image, but only up to a point. If the distance traveled in input space is limited, confidence may max out before reaching $100\%$, regardless of iteration count. Allowing longer distances input space has always worked to convince the network of any desired target class, but images can become visibly adulterated. Regardless, they uncontroversially remain representatives or their source class, just with visible noise-like patterns. The changes in the source image tend to focus on shape-defining features of the contained object. Whether or not a correlation with spatial features of the target class exists remains unclear. In the middle of iterative FGSM the image can cross an indeterminate stage where neither source nor target confidence are greater than those of unrelated classes.

Comparing the spectra of the original data sets and corresponding AEs generated from them revealed discernible differences between the two. One can inspect the distribution of amplitudes with respect to the frequency and orientation, but the former appears to yield more reliable results. In either case, the observed characteristics seem to be dependent on the data set. An investigation involving more networks and data sets could shed light onto this relation.

A noteworthy observation is the inhomogeneity in cross-class conversion compatibility. Even though it seems possible to convert a representative image of any class into any other class by using a sufficiently large step size, conspicuous differences occur when using a limited step size to preserve the image's visual integrity. In that case, select source classes are much more readily transformed to a majority of the possible target classes with high confidence than others. There is a similar effect regarding the target class, some classes are much harder to mimic than others, but the effect is weaker than the one regarding the source class. Another interesting result is that some misclassified original, not-yet-modified images also can't be transformed such that the network predicts the correct class.

An attempt was made to improve the efficiency of adversarial example generation by predicting the final confidence. Fitting a function to the confidence progress has proven to be hard because the initial part of the confidence curve contains little meaningful information. The resulting predictions lead to an unfavorable tradeoff between the low recall and the large required training portion of the progress data.

Adversarial examples also exist outside the domain of images, basically any sufficiently high-dimensional input data space could contain adversarial examples. Humans are very good at inspecting images and noticing visual differences and oddities, so it is reasonable to approach the topic in terms of image classification problems. nonetheless, interesting insights probably await by analyzing adversarial examples outside the image realm. Adversarial examples serve as a reminder that sometimes trying to break a system is a great way to understand how it works, and can suggest ways to further improve it.

% TODO
% Possibility AEs will always exist because homeomorphisms
% Something that might be explored in further work is who AEs generated from training data differ from those spawned from other sources
% Results from section on confusion matrices might be used to more efficiently generate lots of AEs
% Can this AE stuff be done for regression as well?












\begin{appendix}
	\section{Visualizing differences between original image and adver\-sarial image}
	\label{sec:visualizing-diff}
	At various points in the thesis, the difference between an original image taken from a dataset and an adversarial example generated from it are compared. Sometimes this is done by showing the element-wise difference between the two. This, however, poses two challanges: Because the differences are usually tiny, they need to be scaled to visible range, and since we are talking about a difference, negative values also exist and carry meaning. A procedure to produce difference pictures while preserving most of the meaningful information is shown in algorithm \ref{alg:process-difference}.
	
\begin{algorithm}
	\begin{algorithmic}
		\Function{ProcessDifference}{original, adversarial, c, p}
			\State $\text{diff} \gets \frac{255}{c} \cdot \braces{\text{adversarial} - \text{original}}$
			\State posDiff, negDiff $\gets$ diff, $- \text{diff}$
			\State
			\State Set all negative values in posDiff, negDiff to 0
			\State Discard the p-Percentile from posDiff, negDiff
			\State
			\State \Return posDiff, negDiff
		\EndFunction
	\end{algorithmic}
	\caption[Displaying differences between original and adversarial images]{Compute displayable differences between original and adversarial images}
	\label{alg:process-difference}
\end{algorithm}
	
	Here, $c$ is the paramteter of the same name used in generating the AE. In words, the procedure scales the difference to visible range, splits it into a positive and a negative part and discards all values below the p-Percentile to increase lucidity (default $p = 98$). This yields two images, one that shows the changes in the positive direction and one that does the same for the negative direction. Most of the time, not a lot of insight is gained from inspecting both images, so to save space, unless both are shown or otherwise noted, only the positive part will be included, indicating where values have been \emph{added}.
\end{appendix}

\listoffigures
\listoftables
\listofalgorithms

\bibliographystyle{alpha}
\bibliography{ref,image-ref}








\newpage
\topskip 0pt

\vspace*{\fill}

\begin{center}
	\textbf{Declaration}
\end{center}
I declare that the subject of this thesis is not identical to the subject of a thesis handed in by me for another examination.
I furthermore declare that I did not already hand in the thesis at another university for the purpose of attaining an academical degree.

I assert that I authored the thesis independently and did not use any sources except the indicated ones. I indicated the sections of the thesis which were taken from other works in wording or meaning under specification of the source material. This holds analogously for drawings, sketches, pictorial representations and the like.

\vspace{2cm}

\begin{center}
	\textbf{ErklÃ¤rung}
\end{center}
Ich erklÃ¤re, dass das Thema dieser Arbeit nicht identisch ist mit dem Thema einer von mir bereits fÃ¼r eine andere PrÃ¼fung eingereichten Arbeit.
Ich erklÃ¤re weiterhin, dass ich die Arbeit nicht bereits an einer anderen Hochschule zur Erlangung eines akademischen Grades eingereicht habe.

Ich versichere, dass ich die Arbeit selbststÃ¤ndig verfasst und keine anderen als die angegebenen Quellen benutzt habe. Die Stellen der Arbeit, die anderen Werken dem Wortlaut oder dem Sinn nach entnommen sind, habe ich unter Angabe der Quellen der Entlehnung kenntlich gemacht. Dies gilt sinngemÃ¤Ã auch fÃ¼r gelieferte Zeichnungen, Skizzen, bildliche Darstellungen und dergleichen.

\vspace{2cm}

\rule[0.05cm]{5cm}{0.5pt} \hspace{5cm} \rule[0.05cm]{5cm}{0.5pt}\\
Date / Datum \hspace{7.6cm} Signature / Unterschrift

\vspace*{\fill}








\end{document}

% TODO
% List of acronyms

% TODO TODOS
% Be consistent with whether figure captions end with a dot or not.
% Be consistent with whether footnotes end in a dot, and if they start with a capital letter
% Consistent variable names etc.
% Maybe use "dot product" instead of "inner product"?
% Replace can't, don't etc with proper language
% Make sure URL in References show up where important
% Introduction for every section
% Add [h!!!!!!] to a lot of figures
% Remove [scale=...] and [width=...] specifications from figures where unnecessary
% Uniform linewidth of 1.2
% Make sure all plots contain all relevant information
% Rename aes_old folder to aes
% Spell checking
% Rename "original" to source
% Fix undefined references

% AE by superposition
% Is it possible to analyze the dependence of AE generation on data dimensionality by comparing GTSRB-Net and GoogleNet AEs?
% Include paper that was sent to me in email
% Talk about how creating AEs from TRAINING data is reasonable, and maybe even better because its probably harder. (Might also be easier though, since the network is somewhat 'aligned' with the training data. Wait... did i not already talk about this somewhere?

% Source for CNN performance on image processing tasks
% Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106â1114, 2012







